{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models import Model\n",
    "from helper import *\n",
    "import tensorflow as tf, time, ctypes\n",
    "from sparse import COO\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Softmax, ReLU, Flatten, Dense, Dot\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.losses import cosine_similarity, Loss\n",
    "from web.embedding import Embedding\n",
    "from web.evaluate import evaluate_on_all\n",
    "from tensorflow.keras.initializers import Zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.gpu = '0'\n",
    "        self.name = 'test_run'\n",
    "        self.embed_loc = None\n",
    "        self.embed_dim = 300\n",
    "        self.total_sents = 56974869\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = 128\n",
    "        self.max_epochs = 50\n",
    "        self.l2 = 0.00001\n",
    "        self.seed = 1234\n",
    "        self.sample = 1e-4\n",
    "        self.num_neg = 100\n",
    "        self.side_int = 10000\n",
    "        self.gcn_layer = 1\n",
    "        self.dropout = 1.0\n",
    "        self.opt = 'adam'\n",
    "        self.onlyDump = 'store_true'\n",
    "        self.context = False\n",
    "        self.restore ='store_true'\n",
    "        self.emb_dir = './embeddings/'\n",
    "        self.log_dir = './log/'\n",
    "        self.config_dir ='./config/'\n",
    "        self.max_sent_len = 50\n",
    "        self.max_dep_len = 800\n",
    "        self.gamma = 0.1\n",
    "\n",
    "p = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSim(object):\n",
    "   \n",
    "    def __init__(self, sem_info_path, voc2id,\n",
    "                 relu_syn=False, relu_ant=False,\n",
    "                 relu_hyper=False, relu_hypo=False,\n",
    "                 relu_mer=False, gamma=0.1, sample_size=1000):\n",
    "        \n",
    "        # super(SemanticSim, self).__init__()\n",
    "        \n",
    "        self.sem_info_path = sem_info_path\n",
    "        self.voc2id = voc2id\n",
    "        self.relu_syn = relu_syn\n",
    "        self.relu_ant = relu_ant\n",
    "        self.relu_hyper = relu_hyper\n",
    "        self.relu_hypo = relu_hypo\n",
    "        self.relu_mer = relu_mer\n",
    "        self.gamma = gamma\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        self.syn = self._load_file(\"synonyms\")\n",
    "        self.ant = self._load_file(\"antonyms\")\n",
    "        self.hyper = self._load_file(\"hypernyms\")\n",
    "        self.hypo = self._load_file(\"hyponyms\")\n",
    "        self.mer = self._load_file(\"meronyms\")\n",
    "\n",
    "        self.n_samples = len(self.syn) + len(self.ant) + len(self.hyper) + len(self.hypo) + len(self.mer)\n",
    "        if self.sample_size > self.n_samples: \n",
    "            self.sample_size = self.n_samples\n",
    "        self.n_syn = int(len(self.syn) / self.n_samples * self.sample_size) + 1\n",
    "        self.n_ant = int(len(self.ant) / self.n_samples * self.sample_size) + 1\n",
    "        self.n_hyper = int(len(self.hyper) / self.n_samples * self.sample_size) + 1\n",
    "        self.n_hypo = int(len(self.hypo) / self.n_samples * self.sample_size) + 1\n",
    "        self.n_mer = int(len(self.mer) / self.n_samples * self.sample_size) + 1\n",
    "        self.sample_size = self.n_syn + self.n_ant + self.n_hyper + self.n_hypo + self.n_mer\n",
    "           \n",
    "    def _load_file(self, name):\n",
    "        nym = list()\n",
    "        with open(f\"{self.sem_info_path}/{name}.txt\", 'r') as file:\n",
    "            for line in file:\n",
    "                x = line.strip('\\n').split()\n",
    "                inds = list()\n",
    "                for i in x:\n",
    "                    try: inds.append(self.voc2id[i])\n",
    "                    except KeyError: pass\n",
    "                for i in itertools.combinations(inds, 2):\n",
    "                    nym.append(i)\n",
    "            return np.asarray(nym)        \n",
    "            \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        inds = list()\n",
    "        \n",
    "        # Synonyms\n",
    "        ind = np.random.randint(0, len(self.syn), size=(self.n_syn,))\n",
    "        y = tf.gather(x, self.syn[ind], axis=0)\n",
    "        if self.relu_syn:\n",
    "            syn_sim = tf.reduce_sum(tf.nn.relu(cosine_similarity(y[:, 0], y[:, 1])))\n",
    "        else:\n",
    "            syn_sim = tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        inds.append(self.syn[ind])\n",
    "        \n",
    "        # Antonyms\n",
    "        ind = np.random.randint(0, len(self.ant), size=(self.n_ant,))\n",
    "        y = tf.gather(x, self.ant[ind], axis=0)\n",
    "        if self.relu_ant:\n",
    "            ant_sim = tf.reduce_sum(tf.nn.relu(-cosine_similarity(y[:, 0], y[:, 1])))\n",
    "        else:\n",
    "            ant_sim = tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        inds.append(self.ant[ind])\n",
    "        \n",
    "        # Hypernyms\n",
    "        ind = np.random.randint(0, len(self.hyper), size=(self.n_hyper,))\n",
    "        y = tf.gather(x, self.hyper[ind], axis=0)\n",
    "        if self.relu_hyper:\n",
    "            hyper_sim = tf.reduce_sum(tf.nn.relu(cosine_similarity(y[:, 0], y[:, 1])))\n",
    "        else:\n",
    "            hyper_sim = tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        inds.append(self.hyper[ind])\n",
    "        \n",
    "        # Hyponyms\n",
    "        ind = np.random.randint(0, len(self.hypo), size=(self.n_hypo,))\n",
    "        y = tf.gather(x, self.hypo[ind], axis=0)\n",
    "        if self.relu_hypo:\n",
    "            hypo_sim = tf.reduce_sum(tf.nn.relu(cosine_similarity(y[:, 0], y[:, 1])))\n",
    "        else:\n",
    "            hypo_sim = tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        inds.append(self.hypo[ind])\n",
    "        \n",
    "        # Meronyms\n",
    "        ind = np.random.randint(0, len(self.mer), size=(self.n_mer,))\n",
    "        y = tf.gather(x, self.mer[ind], axis=0)\n",
    "        if self.relu_mer:\n",
    "            mer_sim = tf.reduce_sum(tf.nn.relu(cosine_similarity(y[:, 0], y[:, 1])))\n",
    "        else:\n",
    "            mer_sim = tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        inds.append(self.mer[ind])\n",
    "\n",
    "        total = 1 + (syn_sim + ant_sim + hyper_sim + hypo_sim + mer_sim) / self.sample_size\n",
    "        \n",
    "        return inds, self.gamma * total\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'gamma': float(self.gamma), \n",
    "                'sample_size': int(self.sample_size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(Layer):\n",
    "    def __init__(self, vocab, dims, init_file=None, id2voc=None, kernel_regularizer=None, name=None):\n",
    "        \n",
    "        super(WordEmbedding, self).__init__(name=name)\n",
    "        self.vocab = vocab\n",
    "        self.dims = dims\n",
    "        self.id2voc = id2voc\n",
    "        self.init_file = init_file        \n",
    "        self.init_emb = tf.Variable(Zeros()(shape=(self.vocab, self.dims), dtype=tf.float32), trainable=False)        \n",
    "        self.w = self.add_weight(shape=(self.vocab, self.dims),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True, \n",
    "                                 regularizer=kernel_regularizer,\n",
    "                                 dtype=tf.float32)\n",
    "\n",
    "        x = np.asarray(pd.read_csv(self.init_file, sep=' ', header=None))\n",
    "        word2row = {word: int(i) for i, word in enumerate(x[:, 0])}\n",
    "        order = list()\n",
    "        for i in range(len(self.id2voc)):\n",
    "            try:\n",
    "                order.append(word2row[self.id2voc[i]])\n",
    "            except KeyError:\n",
    "                order.append(word2row[\"UNK\"])\n",
    "        x = x[order, 1:]                \n",
    "        self.init_emb.assign(x)\n",
    "        self.w.assign(x)        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.w, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemCorrect(Model):\n",
    "\n",
    "    def __init__(self, vocab_size, dim, voc2id, id2voc, emb_init_file, sem_info_path,\n",
    "                 relu_syn=False, relu_ant=False, relu_hyper=False, relu_hypo=False, relu_mer=False,\n",
    "                 batch_size=128, num_batches=100, gamma=0.1, output_name=None, \n",
    "                 log_dir=\"./log/\", config_dir=\"./config/\"):\n",
    "                \n",
    "        super(SemCorrect, self).__init__()\n",
    "        \n",
    "        self.vocab = vocab_size\n",
    "        self.dim = dim\n",
    "        self.voc2id = voc2id\n",
    "        self.id2voc = id2voc\n",
    "        self.sem_info_path = sem_info_path\n",
    "        self.emb_init_file = emb_init_file\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = num_batches\n",
    "        self.gamma = gamma\n",
    "        self.output_name = output_name\n",
    "        self.log_dir = log_dir\n",
    "        self.config_dir = config_dir\n",
    "        self.relu_syn= relu_syn\n",
    "        self.relu_ant= relu_ant\n",
    "        self.relu_hyper= relu_hyper\n",
    "        self.relu_hypo= relu_hypo\n",
    "        self.relu_mer= relu_mer\n",
    "\n",
    "        self.best_results = 0\n",
    "        self.best_int_avg = 0\n",
    "        \n",
    "        logger = get_logger(self.output_name, self.log_dir, self.config_dir)\n",
    "        logger.setLevel(\"ERROR\")\n",
    "        \n",
    "        self.sem_sim = SemanticSim(self.sem_info_path, self.voc2id, relu_syn=self.relu_syn,\n",
    "                                   relu_ant=self.relu_ant, relu_hyper=self.relu_hyper,\n",
    "                                   relu_hypo=self.relu_hypo, relu_mer=self.relu_mer,\n",
    "                                   gamma=self.gamma, sample_size=self.batch_size)\n",
    "        self.embeddings = WordEmbedding(self.vocab, self.dim, init_file=self.emb_init_file, \n",
    "                                        id2voc=self.id2voc, name=\"word_embeddings\")\n",
    "        self.dot = Dot(axes=-1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        inds, sem_loss = model.sem_sim(model.embeddings.weights[0])        \n",
    "        sample_inds = [i for k in inds for j in k for i in j]\n",
    "        init_emb = tf.expand_dims(model.embeddings.init_emb, 0)\n",
    "        init_sample = tf.expand_dims(tf.nn.embedding_lookup(model.embeddings.init_emb, sample_inds), 0)\n",
    "        curr_sample = tf.expand_dims(model.embeddings(sample_inds), 0)\n",
    "        syn_loss = 1 + tf.reduce_mean(cosine_similarity(model.dot([curr_sample, init_emb]), model.dot([init_sample, init_emb])))\n",
    "        \n",
    "        # Calculate loss.\n",
    "        loss = sem_loss + syn_loss\n",
    "        return loss\n",
    "\n",
    "    def checkpoint(self):\n",
    "        embed_matrix = tf.math.l2_normalize(self.embeddings.weights[0], axis=1)\n",
    "        words = [self.id2voc[i] for i in range(len(self.id2voc))]\n",
    "        voc2vec = dict(zip(words, iter(embed_matrix.numpy())))\n",
    "        embedding = Embedding.from_dict(voc2vec)\n",
    "        results = evaluate_on_all(embedding)\n",
    "        results = {key: round(val[0], 4) for key, val in results.items()}\n",
    "        curr_int = np.mean(list(results.values()))\n",
    "\n",
    "        if curr_int >= self.best_int_avg:\n",
    "            with open(f\"{os.getcwd()}/embeddings/{self.output_name}.txt\", 'w') as file:\n",
    "                for key, values in voc2vec.items():\n",
    "                    file.write(key)\n",
    "                    [file.write(f\" {v}\") for v in values]\n",
    "                    file.write('\\n')\n",
    "            self.best_results = results\n",
    "            self.best_int_avg = curr_int\n",
    "        return results\n",
    "        \n",
    "        \n",
    "    def train(self, epochs):\n",
    "\n",
    "        self.best_int_avg = 0\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}:\\n\")\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "            # Iterate over the batches of the dataset.\n",
    "            for step in range(self.num_batches):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss = model(0)\n",
    "                grads = tape.gradient(loss, self.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "                print(f\"\\rStep: {step + 1}/{self.num_batches}; Elapsed time: {time.time() - start_time:.2f}; \\\n",
    "                      Training loss: {loss:.4f}\", end='\\r') \n",
    "\n",
    "            print(\"Checkpoint:\")\n",
    "            results = self.checkpoint()\n",
    "            print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_info_path = f\"{os.getcwd()}/semantic_info\"\n",
    "emb_init_file = f\"{os.getcwd()}/embeddings/init_syngcn_emb.txt\"\n",
    "\n",
    "voc2id = dict()\n",
    "i = 0\n",
    "with open(emb_init_file, 'r') as file:\n",
    "    for line in file:\n",
    "        voc2id[line.strip().split(' ')[0]] = i\n",
    "        i += 1\n",
    "id2voc = {v: k for k, v in voc2id.items()}\n",
    "vocab_size = len(voc2id)\n",
    "\n",
    "num_batches = 100\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SemCorrect(vocab_size, 300, voc2id, id2voc, emb_init_file, sem_info_path,\n",
    "                   relu_syn=True, relu_ant=True, relu_hyper=True, relu_hypo=True, relu_mer=True,\n",
    "                   batch_size=1000, num_batches=50, gamma=1, output_name=\"fin_syngcn_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
