{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models import Model\n",
    "from helper import *\n",
    "import tensorflow as tf, time, ctypes\n",
    "from sparse import COO\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Softmax, ReLU, Flatten, Dense\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.losses import cosine_similarity, Loss\n",
    "from web.embedding import Embedding\n",
    "from web.evaluate import evaluate_on_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.gpu = '0'\n",
    "        self.name = 'test_run'\n",
    "        self.embed_loc = None\n",
    "        self.embed_dim = 300\n",
    "        self.total_sents = 56974869\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = 128\n",
    "        self.max_epochs = 50\n",
    "        self.l2 = 0.00001\n",
    "        self.seed = 1234\n",
    "        self.sample = 1e-4\n",
    "        self.num_neg = 100\n",
    "        self.side_int = 10000\n",
    "        self.gcn_layer = 1\n",
    "        self.dropout = 1.0\n",
    "        self.opt = 'adam'\n",
    "        self.onlyDump = 'store_true'\n",
    "        self.context = False\n",
    "        self.restore ='store_true'\n",
    "        self.emb_dir = './embeddings/'\n",
    "        self.log_dir = './log/'\n",
    "        self.config_dir ='./config/'\n",
    "        self.max_sent_len = 50\n",
    "        self.max_dep_len = 800\n",
    "        self.gamma = 0.1\n",
    "\n",
    "p = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! g++ batch_generator.cpp -o batchGen.so -fPIC -shared -pthread -O3 -march=native -std=c++11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "\n",
    "voc2id = {k: int(v) for k, v in read_mappings('./data/voc2id.txt').items()}\n",
    "id2freq = id2freq = {int(k): int(v) for k, v in read_mappings('./data/id2freq.txt').items()}\n",
    "id2voc = {v: k for k, v in voc2id.items()}\n",
    "vocab_size = len(voc2id)\n",
    "wrd_list = [id2voc[i] for i in range(vocab_size)]\n",
    "de2id = {k: int(v) for k, v in read_mappings('./data/de2id.txt').items()}\n",
    "num_deLabel = len(de2id)\n",
    "\n",
    "vocab = len(voc2id)\n",
    "corpus_size = np.sum(list(id2freq.values()))\n",
    "voc2freq = [id2freq[_id] for _id in range(len(voc2id))]\n",
    "\n",
    "if not p.context: \n",
    "    p.win_size = 0\n",
    "\n",
    "lib = ctypes.cdll.LoadLibrary('./batchGen.so')\t\t\t# Loads the C++ code for making batches\n",
    "lib.init()\n",
    "\n",
    "# Creating pointers required for creating batches\n",
    "edges = np.zeros(p.max_dep_len * p.batch_size * 3, dtype=np.int32)\n",
    "wrds = np.zeros(p.max_sent_len * p.batch_size, dtype=np.int32)\n",
    "samp = np.zeros(p.max_sent_len * p.batch_size, dtype=np.int32)\n",
    "negs = np.zeros(p.max_sent_len * p.num_neg * p.batch_size, dtype=np.int32)\n",
    "wlen = np.zeros(p.batch_size, dtype=np.int32)\n",
    "elen = np.zeros(p.batch_size, dtype=np.int32)\n",
    "\n",
    "# Pointer address of above arrays\n",
    "edges_addr = edges.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "wrds_addr = wrds.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "negs_addr = negs.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "samp_addr = samp.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "wlen_addr = wlen.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "elen_addr = elen.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    lib.reset()\n",
    "    while True:\n",
    "        eph_ovr = lib.getBatch(edges_addr, wrds_addr, negs_addr, samp_addr, elen_addr, wlen_addr, \n",
    "                        p.win_size, 0, p.batch_size, ctypes.c_float(0))\n",
    "        if eph_ovr == 1: \n",
    "            break\n",
    "        else:\n",
    "            batch = {'edges': edges, 'wrds': wrds, 'elen': elen, 'wlen': wlen}\n",
    "            yield (batch, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.reset()\n",
    "while True:\n",
    "    eph_ovr = lib.getBatch(edges_addr, wrds_addr, negs_addr, samp_addr, elen_addr, wlen_addr, \n",
    "                    p.win_size, 0, p.batch_size, ctypes.c_float(0))\n",
    "    if eph_ovr == 1: \n",
    "        break\n",
    "    else:\n",
    "        batch = {'edges': edges, 'wrds': wrds, 'elen': elen, 'wlen': wlen}\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj(batch, seq_len):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch: Batch returned by getBatch generator\n",
    "    seq_len: Max length of sentence in the batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Adjacency matrix shape=[Number of dependency labels, Batch size, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "\n",
    "    # Total number of edges in batch.\n",
    "    num_edges = np.sum(batch['elen'])\n",
    "    # Sentence number in batch.\n",
    "    b_ind = np.expand_dims(np.repeat(np.arange(p.batch_size), batch['elen']), axis=1)\n",
    "    # Reshape edges vector to have (parent, dep, label) format\n",
    "    e_ind = np.reshape(batch['edges'], [-1, 3])[:num_edges]\n",
    "\n",
    "    # Indexes to format (label, batch_num, parent, dep).\n",
    "    adj_ind = np.concatenate([b_ind, e_ind], axis=1)\n",
    "    adj_ind = adj_ind[:, [3,0,1,2]]\n",
    "    # Put edge weight 1 for each edge in batch.\n",
    "    adj_data = np.ones(num_edges, dtype=np.float32)\n",
    "\n",
    "    return COO(adj_ind.T, adj_data, shape=(num_deLabel, p.batch_size, seq_len, seq_len)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(data, dlen, sub_sample=[]):\n",
    "\n",
    "    \"\"\"\n",
    "    Pads a given batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: List of tokenized sentences in a batch\n",
    "    dlen: Total number of words in each sentence in a batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_pad: Padded word sequence\n",
    "    data_mask: Masking for padded words\n",
    "    max_len: Maximum length of sentence in the batch\n",
    "    \"\"\"\n",
    "    \n",
    "    max_len   = np.max(dlen)\n",
    "    data_pad  = np.zeros([len(dlen), max_len], dtype=np.int32)\n",
    "    data_mask = np.zeros([len(dlen), max_len], dtype=np.float32)\n",
    "\n",
    "    offset = 0\n",
    "    for i in range(len(dlen)):\n",
    "        data_pad [i, :dlen[i]] = data[offset: offset + dlen[i]]\n",
    "        data_mask[i, :dlen[i]] = 1\n",
    "        if len(sub_sample) != 0:\n",
    "            data_mask[i, :dlen[i]] *= sub_sample[offset: offset + dlen[i]]\n",
    "        offset += dlen[i]\n",
    "\n",
    "    return data_pad, data_mask, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Layer):\n",
    "\n",
    "    def __init__(self, out_dim, n_labels, batch_size=128, gating=False, reg=None, name=None):\n",
    "\n",
    "        super(GCN, self).__init__(name=name)\n",
    "        \n",
    "        self.out_dim = out_dim\n",
    "        self.n_labels = n_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.gating = gating\n",
    "        self.reg = reg\n",
    "        \n",
    "        self.in_layers = list()\n",
    "        self.out_layers = list()\n",
    "        self.in_gates = list()\n",
    "        self.out_gates = list()\n",
    "        \n",
    "        for i in range(self.n_labels):\n",
    "            self.in_layers.append(Dense(self.out_dim, kernel_regularizer=self.reg, name=f\"in_{i}\"))\n",
    "            self.out_layers.append(Dense(self.out_dim, kernel_regularizer=self.reg, name=f\"out_{i}\"))\n",
    "            self.in_gates.append(Dense(1, activation=\"sigmoid\", name=f\"in_gate_{i}\"))\n",
    "            self.out_gates.append(Dense(1, activation=\"sigmoid\", name=f\"out_gate_{i}\"))\n",
    "\n",
    "\n",
    "    def call(self, gcn_input, adj_mat):\n",
    "\n",
    "        # Rolling sum of adjacent nodes.\n",
    "        max_nodes = adj_mat.shape[-1]\n",
    "        adj_mat = tf.cast(adj_mat, dtype=tf.float32)\n",
    "        out = tf.zeros([self.batch_size, max_nodes, self.out_dim])\n",
    "        \n",
    "        for i in range(self.n_labels):\n",
    "            # In degrees.\n",
    "            xW_in = self.in_layers[i](gcn_input)\n",
    "            A_in = tf.transpose(adj_mat[i], [0, 2, 1])\n",
    "            if self.gating: \n",
    "                xW_in = xW_in * self.in_gates[i](gcn_input)           \n",
    "            h_in = tf.matmul(A_in, xW_in)\n",
    "            \n",
    "            # Out degrees.\n",
    "            xW_out = self.out_layers[i](gcn_input)\n",
    "            A_out = adj_mat[i]\n",
    "            if self.gating: \n",
    "                xW_out = xW_out * self.out_gates[i](gcn_input)\n",
    "            h_out = tf.matmul(A_out, xW_out)\n",
    "            \n",
    "            # Total\n",
    "            out += (h_in + h_out)\n",
    "            \n",
    "        return out\n",
    "        \n",
    "     \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2MeanRegularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, l2=0.01):\n",
    "        self.l2 = l2\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.l2 * tf.math.reduce_mean(tf.math.square(x))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'l2': float(self.l2)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemRegularizer(Regularizer):\n",
    "    def __init__(self, sem_info_path, voc2id_path, \n",
    "                 gamma=0.1, sample_size=1000):\n",
    "        \n",
    "        self.sem_info_path = sem_info_path\n",
    "        self.voc2id_path = voc2id_path\n",
    "        self.gamma = gamma\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        self.voc2id = dict()\n",
    "        self._load_voc2id()\n",
    "        self.syn = self._load_file(\"synonyms\")\n",
    "        self.ant = self._load_file(\"antonyms\")\n",
    "        self.hyper = self._load_file(\"hypernyms\")\n",
    "        self.hypo = self._load_file(\"hyponyms\")\n",
    "        self.mer = self._load_file(\"meronyms\")\n",
    "\n",
    "        self.n_samples = len(self.syn) + len(self.ant) + len(self.hyper) + len(self.hypo) + len(self.mer)\n",
    "        if self.sample_size > self.n_samples: \n",
    "            self.sample_size = self.n_samples\n",
    "        self.n_syn = int(len(self.syn) / self.n_samples * self.sample_size)\n",
    "        self.n_ant = int(len(self.ant) / self.n_samples * self.sample_size)\n",
    "        self.n_hyper = int(len(self.hyper) / self.n_samples * self.sample_size)\n",
    "        self.n_hypo = int(len(self.hypo) / self.n_samples * self.sample_size)\n",
    "        self.n_mer = int(len(self.mer) / self.n_samples * self.sample_size)\n",
    "        \n",
    "        \n",
    "    def _load_voc2id(self):\n",
    "        with open(self.voc2id_path, 'r') as file:\n",
    "            for line in file:\n",
    "                x = line.strip('\\n').split('\\t')\n",
    "                self.voc2id[x[0]] = int(x[1])        \n",
    "    \n",
    "    def _load_file(self, name):\n",
    "        nym = list()\n",
    "        with open(f\"{self.sem_info_path}/{name}.txt\", 'r') as file:\n",
    "            for line in file:\n",
    "                x = line.strip('\\n').split()\n",
    "                inds = list()\n",
    "                for i in x:\n",
    "                    try: inds.append(self.voc2id[i])\n",
    "                    except KeyError: pass\n",
    "                for i in itertools.combinations(inds, 2):\n",
    "                    nym.append(i)\n",
    "            return np.asarray(nym)        \n",
    "            \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # Synonyms\n",
    "        ind = np.random.randint(0, len(self.syn), size=(self.n_syn,))\n",
    "        y = tf.gather(x, self.syn[ind], axis=0)\n",
    "        syn_sim = self.n_syn + tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        \n",
    "        # Antonyms\n",
    "        ind = np.random.randint(0, len(self.ant), size=(self.n_ant,))\n",
    "        y = tf.gather(x, self.ant[ind], axis=0)\n",
    "        ant_sim = self.n_ant - tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        \n",
    "        # Hypernyms\n",
    "        ind = np.random.randint(0, len(self.hyper), size=(self.n_hyper,))\n",
    "        y = tf.gather(x, self.hyper[ind], axis=0)\n",
    "        hyper_sim = self.n_hyper + tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        \n",
    "        # Hyponyms\n",
    "        ind = np.random.randint(0, len(self.hypo), size=(self.n_hypo,))\n",
    "        y = tf.gather(x, self.hypo[ind], axis=0)\n",
    "        hypo_sim = self.n_hypo + tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        \n",
    "        # Meronyms\n",
    "        ind = np.random.randint(0, len(self.mer), size=(self.n_mer,))\n",
    "        y = tf.gather(x, self.mer[ind], axis=0)\n",
    "        mer_sim = self.n_mer + tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "\n",
    "        total = (syn_sim + ant_sim + hyper_sim + hypo_sim + mer_sim) / self.sample_size\n",
    "        \n",
    "        return self.gamma * total\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'gamma': float(self.gamma), \n",
    "                'sample_size': int(self.sample_size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(Layer):\n",
    "    def __init__(self, vocab, dims, init_file=None, kernel_regularizer=None, name=None):\n",
    "        \n",
    "        super(WordEmbedding, self).__init__(name=name)\n",
    "        self.vocab = vocab\n",
    "        self.dims = dims\n",
    "        self.init_file = init_file\n",
    "        \n",
    "        self.w = self.add_weight(shape=(self.vocab, self.dims),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True, \n",
    "                                 regularizer=kernel_regularizer)\n",
    "\n",
    "        if self.init_file != None:            \n",
    "            x = tf.Variable(initial_value=np.asarray(pd.read_csv(self.init_file, sep=' ', header=None).iloc[:, 1:], dtype=\"float32\"), \n",
    "                                 dtype=tf.float32, trainable=True)\n",
    "            self.w.assign(x)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.w, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynGCN(Model):\n",
    "\n",
    "    def __init__(self, vocab, dim, num_deLabel, id2voc, voc2freq, sem_info_path, voc2id_path, \n",
    "                 batch_size=128, emb_init_file=None, gcn_gating=True,\n",
    "                 gamma=0.1, reg_sample_size=1000, num_neg=100, name=None, \n",
    "                 log_dir=\"./log/\", config_dir=\"./config/\"):\n",
    "                \n",
    "        super(SynGCN, self).__init__()\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.dim = dim\n",
    "        self.sem_info_path = sem_info_path\n",
    "        self.voc2id_path = voc2id_path\n",
    "        self.emb_init_file = emb_init_file\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.reg_sample_size = reg_sample_size\n",
    "        self.num_deLabel = num_deLabel\n",
    "        self.gcn_gating=gcn_gating\n",
    "        self.voc2freq = voc2freq\n",
    "        self.num_neg = num_neg\n",
    "        self.id2voc = id2voc\n",
    "        self.best_results = 0\n",
    "        self.best_int_avg = 0\n",
    "        self.output_name = name\n",
    "        self.log_dir = log_dir\n",
    "        self.config_dir = config_dir\n",
    "        \n",
    "        logger = get_logger(self.output_name, self.log_dir, self.config_dir)\n",
    "        logger.setLevel(\"ERROR\")\n",
    "        \n",
    "        self.l2_reg = L2MeanRegularizer(l2=0.1)\n",
    "        self.sem_reg = SemRegularizer(self.sem_info_path, self.voc2id_path, \n",
    "                                      gamma=self.gamma, sample_size=self.reg_sample_size)\n",
    "        self.embeddings = WordEmbedding(self.vocab, self.dim, init_file=self.emb_init_file, \n",
    "                                    kernel_regularizer=self.sem_reg, name=\"word_embeddings\")\n",
    "        self.gcn = GCN(self.dim, self.num_deLabel, batch_size=self.batch_size, gating=self.gcn_gating, \n",
    "                      reg=self.l2_reg)\n",
    "        self.relu = ReLU()\n",
    "        self.context = WordEmbedding(self.vocab, self.dim, init_file=None, \n",
    "                                 name=\"context_embeddings\", kernel_regularizer=self.l2_reg)        \n",
    "        self.bin_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        \n",
    "    def call(self, batch):\n",
    "        \n",
    "        # -- Preprocess batch data -- #\n",
    "        # Pad sentences and sample words mask.\n",
    "        words_pad, words_mask, seq_len = pad_data(batch['wrds'], batch['wlen'])\n",
    "        # Gold label indices.\n",
    "        target_words = tf.cast(tf.reshape(words_pad, [-1, 1]), tf.int64)\n",
    "        # Get adjacency matrix.\n",
    "        adj_mat = get_adj(batch, seq_len)        \n",
    "        # Get negative samples.\n",
    "        neg_ids, _, _ = tf.nn.fixed_unigram_candidate_sampler(true_classes=target_words, num_true=1,\n",
    "                                                              num_sampled=self.num_neg * self.batch_size,\n",
    "                                                              unique=True, distortion=0.75, \n",
    "                                                              range_max=self.vocab, unigrams=self.voc2freq)\n",
    "        neg_ids = tf.cast(neg_ids, dtype=tf.int32)\n",
    "        neg_ids = tf.reshape(neg_ids, [self.batch_size, self.num_neg])\n",
    "        neg_ids = tf.reshape(tf.tile(neg_ids, [1, seq_len]), [self.batch_size, seq_len, self.num_neg])        \n",
    "        # Concatenate true word and negative samples.\n",
    "        target_ind = tf.concat([tf.expand_dims(words_pad, axis=2), neg_ids], axis=2)\n",
    "        # Assign true labels.\n",
    "        target_labels = tf.concat([tf.ones([self.batch_size, seq_len, 1], dtype=tf.float32), \n",
    "                                   tf.zeros([self.batch_size, seq_len, self.num_neg], dtype=tf.float32)], \n",
    "                                  axis=2) * tf.expand_dims(words_mask, -1)\n",
    "        \n",
    "        # -- Run model -- #\n",
    "        gcn_out = self.gcn(self.embeddings(words_pad), adj_mat)\n",
    "        target_embed = self.context(target_ind)\n",
    "        pred = tf.reduce_sum(tf.expand_dims(gcn_out, axis=2) * \n",
    "                             target_embed, axis=3) * tf.expand_dims(words_mask, -1)        \n",
    "        # Calculate loss.\n",
    "        loss = self.bin_loss(tf.reshape(target_labels, -1), tf.reshape(pred, -1))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def checkpoint(self):\n",
    "        embed_matrix = tf.math.l2_normalize(self.embeddings.weights[0], axis=1)\n",
    "        words = [self.id2voc[i] for i in range(len(self.id2voc))]\n",
    "        voc2vec = dict(zip(words, iter(embed_matrix.numpy())))\n",
    "        embedding = Embedding.from_dict(voc2vec)\n",
    "        results = evaluate_on_all(embedding)\n",
    "        results = {key: round(val[0], 4) for key, val in results.items()}\n",
    "        curr_int = np.mean(list(results.values()))\n",
    "        # self.logger.info('Current Score: {}'.format(curr_int))\n",
    "\n",
    "        if curr_int >= self.best_int_avg:\n",
    "            # self.logger.info(\"Saving embedding matrix\")\n",
    "            with open(f\"{os.getcwd()}/embeddings/{self.output_name}.txt\", 'w') as file:\n",
    "                for key, values in voc2vec.items():\n",
    "                    file.write(key)\n",
    "                    [file.write(f\" {v}\") for v in values]\n",
    "                    file.write('\\n')\n",
    "            self.best_results = results\n",
    "            self.best_int_avg = curr_int\n",
    "\n",
    "\n",
    "    def train(self, epochs):\n",
    "\n",
    "        self.best_int_avg = 0\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            num_batches = get_corpus_len() // self.batch_size\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}:\\n\")\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "            # Iterate over the batches of the dataset.\n",
    "            for step, (X, y) in enumerate(get_batch()):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss = model(X)\n",
    "                    # print(loss, tf.reduce_sum(model.losses))\n",
    "                    loss += tf.reduce_sum(model.losses)\n",
    "\n",
    "                grads = tape.gradient(loss, self.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "                print(f\"\\rStep: {step + 1}/{num_batches}; Elapsed time: {time.time() - start_time:.2f}; \\\n",
    "                      Training loss: {loss:.4f}\", end='\\r') \n",
    "\n",
    "#                 # Checkpoint every 100 batches.\n",
    "#                 if step % 100 == 0:\n",
    "#                     print(f\"Step: {step}/{num_batches}; Training loss: {loss:.4f}\")\n",
    "\n",
    "            # At end of each epoch.\n",
    "            self.checkpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_info_path = f\"{os.getcwd()}/semantic_info\"\n",
    "voc2id_path = f\"{os.getcwd()}/data/voc2id.txt\"\n",
    "emb_init_file = f\"{os.getcwd()}/embeddings/init_syngcn_emb.txt\"\n",
    "model = SynGCN(vocab, p.embed_dim, num_deLabel, id2voc, voc2freq, sem_info_path, voc2id_path, \n",
    "                 batch_size=128, emb_init_file=emb_init_file, gcn_gating=True,\n",
    "                 gamma=0.1, reg_sample_size=1000, num_neg=100, name=\"syngcn_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1:\n",
      "\n",
      "Step: 7/443676; Elapsed time: 30.67;                       Training loss: 0.7991\r"
     ]
    }
   ],
   "source": [
    "model.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AP': 0.6791,\n",
       " 'BLESS': 0.81,\n",
       " 'Battig': 0.4636,\n",
       " 'ESSLI_2c': 0.6444,\n",
       " 'ESSLI_2b': 0.825,\n",
       " 'ESSLI_1a': 0.7273,\n",
       " 'MEN': 0.7103,\n",
       " 'WS353': 0.6089,\n",
       " 'WS353R': 0.4856,\n",
       " 'WS353S': 0.7501,\n",
       " 'SimLex999': 0.4477,\n",
       " 'RW': 0.3359,\n",
       " 'RG65': 0.7962,\n",
       " 'MTurk': 0.6625,\n",
       " 'Google': 0.5073,\n",
       " 'MSR': 0.5284,\n",
       " 'SemEval2012_2': 0.2268}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56790634"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_corpus_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AP': 0.1592,\n",
       " 'BLESS': 0.22,\n",
       " 'Battig': 0.0879,\n",
       " 'ESSLI_2c': 0.3111,\n",
       " 'ESSLI_2b': 0.425,\n",
       " 'ESSLI_1a': 0.3409,\n",
       " 'MEN': 0.0225,\n",
       " 'WS353': 0.1251,\n",
       " 'WS353R': 0.1523,\n",
       " 'WS353S': 0.1469,\n",
       " 'SimLex999': 0.0316,\n",
       " 'RW': 0.0028,\n",
       " 'RG65': -0.0509,\n",
       " 'MTurk': -0.0172,\n",
       " 'Google': 0.0,\n",
       " 'MSR': 0.0,\n",
       " 'SemEval2012_2': 0.027}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = tf.math.l2_normalize(model.embeddings.weights[0], axis=1)\n",
    "words = [model.id2voc[i] for i in range(len(model.id2voc))]\n",
    "voc2vec = dict(zip(words, iter(embed_matrix.numpy())))\n",
    "w = Embedding.from_dict(voc2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-c77ec48d874f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_on_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Project/COMS_W4995_Project/web/evaluate.py\u001b[0m in \u001b[0;36mevaluate_on_all\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0manalogy_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalogy_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0manalogy_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_analogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Analogy prediction accuracy on {} {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalogy_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/COMS_W4995_Project/web/evaluate.py\u001b[0m in \u001b[0;36mevaluate_analogy\u001b[0;34m(w, X, y, method, k, category, batch_size)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleAnalogySolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/COMS_W4995_Project/web/analogy.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mD_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = evaluate_on_all(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecbm4040/anaconda3/envs/envTF22/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mean_vector = np.mean(w.vectors, axis=0, keepdims=True)\n",
    "words = np.vstack(w.get(word, mean_vector) for word in X.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(np.asarray(list(voc2id.keys())[:100]), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.asarray([w.get(word, mean_vector) for word in X.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.059 , -0.0302, -0.0664, ..., -0.0599,  0.0518,  0.0642],\n",
       "       [ 0.0722,  0.0385,  0.0341, ...,  0.0603, -0.0559, -0.0794],\n",
       "       [-0.0555,  0.0458,  0.053 , ...,  0.0658, -0.0559, -0.0762],\n",
       "       ...,\n",
       "       [ 0.0761, -0.0599, -0.0586, ...,  0.0498, -0.0547,  0.0661],\n",
       "       [ 0.0621, -0.0275, -0.0333, ...,  0.0198, -0.0061, -0.0394],\n",
       "       [ 0.0926, -0.0244, -0.0673, ...,  0.0379, -0.0509, -0.0206]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.059 , -0.0302, -0.0664, ..., -0.0599,  0.0518,  0.0642],\n",
       "       [ 0.0722,  0.0385,  0.0341, ...,  0.0603, -0.0559, -0.0794],\n",
       "       [-0.0555,  0.0458,  0.053 , ...,  0.0658, -0.0559, -0.0762],\n",
       "       ...,\n",
       "       [ 0.0761, -0.0599, -0.0586, ...,  0.0498, -0.0547,  0.0661],\n",
       "       [ 0.0621, -0.0275, -0.0333, ...,  0.0198, -0.0061, -0.0394],\n",
       "       [ 0.0926, -0.0244, -0.0673, ...,  0.0379, -0.0509, -0.0206]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def __init__(self, params):\n",
    "    \"\"\"\n",
    "    Constructor for the main function. Loads data and creates computation graph. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params:\t\tHyperparameters of the model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    self.p = params\n",
    "\n",
    "    if not os.path.isdir(self.p.log_dir): os.system('mkdir {}'.format(self.p.log_dir))\n",
    "    if not os.path.isdir(self.p.emb_dir): os.system('mkdir {}'.format(self.p.emb_dir))\n",
    "\n",
    "    self.logger = get_logger(self.name, self.log_dir, self.config_dir)\n",
    "\n",
    "\n",
    "    self.logger.info(vars(self.p)); pprint(vars(self.p))\n",
    "    self.p.batch_size = self.p.batch_size\n",
    "\n",
    "    if self.p.l2 == 0.0:    self.regularizer = None\n",
    "    else:           \tself.regularizer = tf.contrib.layers.l2_regularizer(scale=self.p.l2)\n",
    "\n",
    "    self.load_data()\n",
    "    self.add_placeholders()\n",
    "\n",
    "    nn_out    = self.add_model()\n",
    "    self.loss = self.add_loss_op(nn_out)\n",
    "\n",
    "    if self.p.opt == 'adam': self.train_op = self.add_optimizer(self.loss)\n",
    "    else:            self.train_op = self.add_optimizer(self.loss, isAdam=False)\n",
    "\n",
    "    self.merged_summ = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "def run_epoch(self, sess, epoch, shuffle=True):\n",
    "    losses = []\n",
    "    cnt = 0\n",
    "\n",
    "    st = time.time()\n",
    "    for step, batch in enumerate(self.getBatches(shuffle)):\n",
    "        feed    = self.create_feed_dict(batch)\n",
    "        loss, _ = sess.run([self.loss, self.train_op], feed_dict=feed)\n",
    "        losses.append(loss)\n",
    "        cnt += self.p.batch_size\n",
    "\n",
    "        if (step+1) % 10 == 0:\n",
    "            self.logger.info('E:{} (Sents: {}/{} [{}]): Train Loss \\t{:.5}\\t{}\\t{:.5}'.format(epoch, cnt, self.p.total_sents, round(cnt/self.p.total_sents * 100 , 1), np.mean(losses), self.p.name, self.best_int_avg))\n",
    "            en = time.time()\n",
    "            if (en-st) >= (3600):\n",
    "                self.logger.info(\"One more hour is over\")\n",
    "                self.checkpoint(epoch, sess)\n",
    "                st = time.time()\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model.fit(sess)\n",
    "\n",
    "print('Model Trained Successfully!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a generator for creating batches\n",
    "Parameters\n",
    "----------\n",
    "shuffle:\tWhether to shuffle batches or not\n",
    "Returns\n",
    "-------\n",
    "A batch in the form of a diciontary\n",
    "    edges:\tDependency parse edges\n",
    "    wrds:\tWord in the batch\n",
    "    negs:\tList of negative samples\n",
    "    sample: Subsampled words indicator\n",
    "    elen:\tTotal number of edges in each sentence\n",
    "    wlen:\tTotal number of words in each sentence\n",
    "\"\"\"\n",
    "lib.reset()\n",
    "\n",
    "while True:\n",
    "    # max_len = 0; unused variable\n",
    "    eph_ovr = lib.getBatch(edges_addr, wrds_addr, negs_addr, samp_addr, elen_addr, wlen_addr, \n",
    "                    3, num_neg, batch_size, ctypes.c_float(1e-4))\n",
    "    if eph_ovr == 1: break\n",
    "    x = {'edges': edges, 'wrds': wrds, 'negs': negs, 'sample': samp, 'elen': elen, 'wlen': wlen}\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0, 26,  5,  1, 11,  5,  2, 23,  5,  3, 34,  5,  4,  7,  7,  6,\n",
       "       11,  5,  7,  9,  9,  8,  7,  7,  9, 38,  9, 10, 13, 13, 11,  2, 13,\n",
       "       12,  7, 10, 13, 16,  5, 14, 10,  3,  0,  3,  3,  1, 21,  3,  2, 15,\n",
       "        6,  4,  2,  6,  5,  7,  3,  6, 16,  3,  7, 10, 12,  8,  0, 10,  9,\n",
       "        7, 12, 10, 11, 12, 11, 27,  3, 12, 17, 12, 13, 38, 15, 14, 15, 17,\n",
       "       15, 15, 17, 16,  2, 12, 17, 16, 17, 18, 13, 20, 19,  2, 18],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['edges'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0, 26,  5,  1, 11,  5,  2, 23,  5,  3, 34,  5,  4,  7,  7,  6,\n",
       "       11,  5,  7,  9,  9,  8,  7,  7,  9, 38,  9, 10, 13, 13, 11,  2, 13,\n",
       "       12,  7, 10, 13, 16,  5, 14, 10], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['edges'][:42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
