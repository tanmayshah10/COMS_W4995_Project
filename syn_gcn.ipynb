{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models import Model\n",
    "from helper import *\n",
    "import tensorflow as tf, time, ctypes\n",
    "from sparse import COO\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Softmax, ReLU, Flatten, Dense\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.losses import cosine_similarity, Loss\n",
    "from web.embedding import Embedding\n",
    "from web.evaluate import evaluate_on_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.gpu = '0'\n",
    "        self.name = 'test_run'\n",
    "        self.embed_loc = None\n",
    "        self.embed_dim = 300\n",
    "        self.total_sents = 56974869\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = 128\n",
    "        self.max_epochs = 50\n",
    "        self.l2 = 0.00001\n",
    "        self.seed = 1234\n",
    "        self.sample = 1e-4\n",
    "        self.num_neg = 100\n",
    "        self.side_int = 10000\n",
    "        self.gcn_layer = 1\n",
    "        self.dropout = 1.0\n",
    "        self.opt = 'adam'\n",
    "        self.onlyDump = 'store_true'\n",
    "        self.context = False\n",
    "        self.restore ='store_true'\n",
    "        self.emb_dir = './embeddings/'\n",
    "        self.log_dir = './log/'\n",
    "        self.config_dir ='./config/'\n",
    "        self.max_sent_len = 50\n",
    "        self.max_dep_len = 800\n",
    "        self.gamma = 0.1\n",
    "\n",
    "p = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! g++ batch_generator.cpp -o batchGen.so -fPIC -shared -pthread -O3 -march=native -std=c++11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "\n",
    "voc2id = {k: int(v) for k, v in read_mappings('./data/voc2id.txt').items()}\n",
    "id2freq = id2freq = {int(k): int(v) for k, v in read_mappings('./data/id2freq.txt').items()}\n",
    "id2voc = {v: k for k, v in voc2id.items()}\n",
    "vocab_size = len(voc2id)\n",
    "wrd_list = [id2voc[i] for i in range(vocab_size)]\n",
    "de2id = {k: int(v) for k, v in read_mappings('./data/de2id.txt').items()}\n",
    "num_deLabel = len(de2id)\n",
    "\n",
    "vocab = len(voc2id)\n",
    "corpus_size = np.sum(list(id2freq.values()))\n",
    "voc2freq = [id2freq[_id] for _id in range(len(voc2id))]\n",
    "\n",
    "if not p.context: \n",
    "    p.win_size = 0\n",
    "\n",
    "lib = ctypes.cdll.LoadLibrary('./batchGen.so')\t\t\t# Loads the C++ code for making batches\n",
    "lib.init()\n",
    "\n",
    "# Creating pointers required for creating batches\n",
    "edges = np.zeros(p.max_dep_len * p.batch_size * 3, dtype=np.int32)\n",
    "wrds = np.zeros(p.max_sent_len * p.batch_size, dtype=np.int32)\n",
    "samp = np.zeros(p.max_sent_len * p.batch_size, dtype=np.int32)\n",
    "negs = np.zeros(p.max_sent_len * p.num_neg * p.batch_size, dtype=np.int32)\n",
    "wlen = np.zeros(p.batch_size, dtype=np.int32)\n",
    "elen = np.zeros(p.batch_size, dtype=np.int32)\n",
    "\n",
    "# Pointer address of above arrays\n",
    "edges_addr = edges.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "wrds_addr = wrds.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "negs_addr = negs.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "samp_addr = samp.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "wlen_addr = wlen.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "elen_addr = elen.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    lib.reset()\n",
    "    while True:\n",
    "        eph_ovr = lib.getBatch(edges_addr, wrds_addr, negs_addr, samp_addr, elen_addr, wlen_addr, \n",
    "                        p.win_size, 0, p.batch_size, ctypes.c_float(0))\n",
    "        if eph_ovr == 1: \n",
    "            break\n",
    "        else:\n",
    "            batch = {'edges': edges, 'wrds': wrds, 'elen': elen, 'wlen': wlen}\n",
    "            yield (batch, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.reset()\n",
    "while True:\n",
    "    eph_ovr = lib.getBatch(edges_addr, wrds_addr, negs_addr, samp_addr, elen_addr, wlen_addr, \n",
    "                    p.win_size, 0, p.batch_size, ctypes.c_float(0))\n",
    "    if eph_ovr == 1: \n",
    "        break\n",
    "    else:\n",
    "        batch = {'edges': edges, 'wrds': wrds, 'elen': elen, 'wlen': wlen}\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj(batch, seq_len):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch: Batch returned by getBatch generator\n",
    "    seq_len: Max length of sentence in the batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Adjacency matrix shape=[Number of dependency labels, Batch size, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "\n",
    "    # Total number of edges in batch.\n",
    "    num_edges = np.sum(batch['elen'])\n",
    "    # Sentence number in batch.\n",
    "    b_ind = np.expand_dims(np.repeat(np.arange(p.batch_size), batch['elen']), axis=1)\n",
    "    # Reshape edges vector to have (parent, dep, label) format\n",
    "    e_ind = np.reshape(batch['edges'], [-1, 3])[:num_edges]\n",
    "\n",
    "    # Indexes to format (label, batch_num, parent, dep).\n",
    "    adj_ind = np.concatenate([b_ind, e_ind], axis=1)\n",
    "    adj_ind = adj_ind[:, [3,0,1,2]]\n",
    "    # Put edge weight 1 for each edge in batch.\n",
    "    adj_data = np.ones(num_edges, dtype=np.float32)\n",
    "\n",
    "    return COO(adj_ind.T, adj_data, shape=(num_deLabel, p.batch_size, seq_len, seq_len)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(data, dlen, sub_sample=[]):\n",
    "\n",
    "    \"\"\"\n",
    "    Pads a given batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: List of tokenized sentences in a batch\n",
    "    dlen: Total number of words in each sentence in a batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_pad: Padded word sequence\n",
    "    data_mask: Masking for padded words\n",
    "    max_len: Maximum length of sentence in the batch\n",
    "    \"\"\"\n",
    "    \n",
    "    max_len   = np.max(dlen)\n",
    "    data_pad  = np.zeros([len(dlen), max_len], dtype=np.int32)\n",
    "    data_mask = np.zeros([len(dlen), max_len], dtype=np.float32)\n",
    "\n",
    "    offset = 0\n",
    "    for i in range(len(dlen)):\n",
    "        data_pad [i, :dlen[i]] = data[offset: offset + dlen[i]]\n",
    "        data_mask[i, :dlen[i]] = 1\n",
    "        if len(sub_sample) != 0:\n",
    "            data_mask[i, :dlen[i]] *= sub_sample[offset: offset + dlen[i]]\n",
    "        offset += dlen[i]\n",
    "\n",
    "    return data_pad, data_mask, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Layer):\n",
    "\n",
    "    def __init__(self, out_dim, n_labels, batch_size=128, gating=False, reg=None, name=None):\n",
    "\n",
    "        super(GCN, self).__init__(name=name)\n",
    "        \n",
    "        self.out_dim = out_dim\n",
    "        self.n_labels = n_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.gating = gating\n",
    "        self.reg = reg\n",
    "        \n",
    "        self.in_layers = list()\n",
    "        self.out_layers = list()\n",
    "        self.in_gates = list()\n",
    "        self.out_gates = list()\n",
    "        \n",
    "        for i in range(self.n_labels):\n",
    "            self.in_layers.append(Dense(self.out_dim, kernel_regularizer=self.reg, name=f\"in_{i}\"))\n",
    "            self.out_layers.append(Dense(self.out_dim, kernel_regularizer=self.reg, name=f\"out_{i}\"))\n",
    "            self.in_gates.append(Dense(1, activation=\"sigmoid\", name=f\"in_gate_{i}\"))\n",
    "            self.out_gates.append(Dense(1, activation=\"sigmoid\", name=f\"out_gate_{i}\"))\n",
    "\n",
    "\n",
    "    def call(self, gcn_input, adj_mat):\n",
    "\n",
    "        # Rolling sum of adjacent nodes.\n",
    "        max_nodes = adj_mat.shape[-1]\n",
    "        adj_mat = tf.cast(adj_mat, dtype=tf.float32)\n",
    "        out = tf.zeros([self.batch_size, max_nodes, self.out_dim])\n",
    "        \n",
    "        for i in range(self.n_labels):\n",
    "            # In degrees.\n",
    "            xW_in = self.in_layers[i](gcn_input)\n",
    "            A_in = tf.transpose(adj_mat[i], [0, 2, 1])\n",
    "            if self.gating: \n",
    "                xW_in = xW_in * self.in_gates[i](gcn_input)           \n",
    "            h_in = tf.matmul(A_in, xW_in)\n",
    "            \n",
    "            # Out degrees.\n",
    "            xW_out = self.out_layers[i](gcn_input)\n",
    "            A_out = adj_mat[i]\n",
    "            if self.gating: \n",
    "                xW_out = xW_out * self.out_gates[i](gcn_input)\n",
    "            h_out = tf.matmul(A_out, xW_out)\n",
    "            \n",
    "            # Total\n",
    "            out += (h_in + h_out)\n",
    "            \n",
    "        return out\n",
    "        \n",
    "     \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2MeanRegularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, l2=0.01):\n",
    "        self.l2 = l2\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.l2 * tf.math.reduce_mean(tf.math.square(x))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'l2': float(self.l2)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemRegularizer(Regularizer):\n",
    "    def __init__(self, sem_info_path, voc2id_path, \n",
    "                 gamma=0.1, sample_size=1000):\n",
    "        \n",
    "        self.sem_info_path = sem_info_path\n",
    "        self.voc2id_path = voc2id_path\n",
    "        self.gamma = gamma\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        self.voc2id = dict()\n",
    "        self._load_voc2id()\n",
    "        self.syn = self._load_file(\"synonyms\")\n",
    "        self.ant = self._load_file(\"antonyms\")\n",
    "        self.hyper = self._load_file(\"hypernyms\")\n",
    "        self.hypo = self._load_file(\"hyponyms\")\n",
    "        self.mer = self._load_file(\"meronyms\")\n",
    "\n",
    "        self.n_samples = len(self.syn) + len(self.ant) + len(self.hyper) + len(self.hypo) + len(self.mer)\n",
    "        if self.sample_size > self.n_samples: \n",
    "            self.sample_size = self.n_samples\n",
    "        self.n_syn = int(len(self.syn) / self.n_samples * self.sample_size)\n",
    "        self.n_ant = int(len(self.ant) / self.n_samples * self.sample_size)\n",
    "        self.n_hyper = int(len(self.hyper) / self.n_samples * self.sample_size)\n",
    "        self.n_hypo = int(len(self.hypo) / self.n_samples * self.sample_size)\n",
    "        self.n_mer = int(len(self.mer) / self.n_samples * self.sample_size)\n",
    "        \n",
    "        \n",
    "    def _load_voc2id(self):\n",
    "        with open(self.voc2id_path, 'r') as file:\n",
    "            for line in file:\n",
    "                x = line.strip('\\n').split('\\t')\n",
    "                self.voc2id[x[0]] = int(x[1])        \n",
    "    \n",
    "    def _load_file(self, name):\n",
    "        nym = list()\n",
    "        with open(f\"{self.sem_info_path}/{name}.txt\", 'r') as file:\n",
    "            for line in file:\n",
    "                x = line.strip('\\n').split()\n",
    "                inds = list()\n",
    "                for i in x:\n",
    "                    try: inds.append(self.voc2id[i])\n",
    "                    except KeyError: pass\n",
    "                for i in itertools.combinations(inds, 2):\n",
    "                    nym.append(i)\n",
    "            return np.asarray(nym)        \n",
    "            \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # Synonyms\n",
    "        ind = np.random.randint(0, len(self.syn), size=(self.n_syn,))\n",
    "        y = tf.gather(x, self.syn[ind], axis=0)\n",
    "        syn_sim = self.n_syn + tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        \n",
    "        # Antonyms\n",
    "        ind = np.random.randint(0, len(self.ant), size=(self.n_ant,))\n",
    "        y = tf.gather(x, self.ant[ind], axis=0)\n",
    "        ant_sim = self.n_ant - tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        \n",
    "        # Hypernyms\n",
    "        ind = np.random.randint(0, len(self.hyper), size=(self.n_hyper,))\n",
    "        y = tf.gather(x, self.hyper[ind], axis=0)\n",
    "        hyper_sim = self.n_hyper + tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        \n",
    "        # Hyponyms\n",
    "        ind = np.random.randint(0, len(self.hypo), size=(self.n_hypo,))\n",
    "        y = tf.gather(x, self.hypo[ind], axis=0)\n",
    "        hypo_sim = self.n_hypo + tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        \n",
    "        # Meronyms\n",
    "        ind = np.random.randint(0, len(self.mer), size=(self.n_mer,))\n",
    "        y = tf.gather(x, self.mer[ind], axis=0)\n",
    "        mer_sim = self.n_mer + tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "\n",
    "        total = (syn_sim + ant_sim + hyper_sim + hypo_sim + mer_sim) / self.sample_size\n",
    "        \n",
    "        return self.gamma * total\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'gamma': float(self.gamma), \n",
    "                'sample_size': int(self.sample_size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(Layer):\n",
    "    def __init__(self, vocab, dims, init_file=None, id2voc=None, kernel_regularizer=None, name=None):\n",
    "        \n",
    "        super(WordEmbedding, self).__init__(name=name)\n",
    "        self.vocab = vocab\n",
    "        self.dims = dims\n",
    "        self.id2voc = id2voc\n",
    "        self.init_file = init_file\n",
    "        \n",
    "        \n",
    "        self.w = self.add_weight(shape=(self.vocab, self.dims),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True, \n",
    "                                 regularizer=kernel_regularizer)\n",
    "\n",
    "        if self.init_file != None and self.id2voc != None:            \n",
    "            x = np.asarray(pd.read_csv(self.init_file, sep=' ', header=None))\n",
    "            word2row = {word: int(i) for i, word in enumerate(x[:, 0])}\n",
    "            order = list()\n",
    "            for i in range(len(self.id2voc)):\n",
    "                try:\n",
    "                    order.append(word2row[self.id2voc[i]])\n",
    "                except KeyError:\n",
    "                    order.append(word2row[\"UNK\"])\n",
    "            x = x[order, 1:]                        \n",
    "            self.w.assign(x)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.w, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynGCN(Model):\n",
    "\n",
    "    def __init__(self, vocab, dim, num_deLabel, id2voc, voc2freq, sem_info_path, voc2id_path, \n",
    "                 batch_size=128, emb_init_file=None, gcn_gating=True,\n",
    "                 gamma=0.1, reg_sample_size=1000, num_neg=100, name=None, \n",
    "                 log_dir=\"./log/\", config_dir=\"./config/\"):\n",
    "                \n",
    "        super(SynGCN, self).__init__()\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.dim = dim\n",
    "        self.sem_info_path = sem_info_path\n",
    "        self.voc2id_path = voc2id_path\n",
    "        self.emb_init_file = emb_init_file\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.reg_sample_size = reg_sample_size\n",
    "        self.num_deLabel = num_deLabel\n",
    "        self.gcn_gating=gcn_gating\n",
    "        self.voc2freq = voc2freq\n",
    "        self.num_neg = num_neg\n",
    "        self.id2voc = id2voc\n",
    "        self.best_results = 0\n",
    "        self.best_int_avg = 0\n",
    "        self.output_name = name\n",
    "        self.log_dir = log_dir\n",
    "        self.config_dir = config_dir\n",
    "        \n",
    "        logger = get_logger(self.output_name, self.log_dir, self.config_dir)\n",
    "        logger.setLevel(\"ERROR\")\n",
    "        \n",
    "        self.l2_reg = L2MeanRegularizer(l2=0.1)\n",
    "        self.sem_reg = SemRegularizer(self.sem_info_path, self.voc2id_path, \n",
    "                                      gamma=self.gamma, sample_size=self.reg_sample_size)\n",
    "        self.embeddings = WordEmbedding(self.vocab, self.dim, init_file=self.emb_init_file, \n",
    "                                        id2voc=self.id2voc, kernel_regularizer=self.sem_reg, \n",
    "                                        name=\"word_embeddings\")\n",
    "        self.gcn = GCN(self.dim, self.num_deLabel, batch_size=self.batch_size, gating=self.gcn_gating, \n",
    "                      reg=self.l2_reg)\n",
    "        self.relu = ReLU()\n",
    "        self.context = WordEmbedding(self.vocab, self.dim, init_file=None, id2voc=None,\n",
    "                                 name=\"context_embeddings\", kernel_regularizer=self.l2_reg)        \n",
    "        self.bin_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        \n",
    "    def call(self, batch):\n",
    "        \n",
    "        # -- Preprocess batch data -- #\n",
    "        # Pad sentences and sample words mask.\n",
    "        words_pad, words_mask, seq_len = pad_data(batch['wrds'], batch['wlen'])\n",
    "        # Gold label indices.\n",
    "        target_words = tf.cast(tf.reshape(words_pad, [-1, 1]), tf.int64)\n",
    "        # Get adjacency matrix.\n",
    "        adj_mat = get_adj(batch, seq_len)        \n",
    "        # Get negative samples.\n",
    "        neg_ids, _, _ = tf.nn.fixed_unigram_candidate_sampler(true_classes=target_words, num_true=1,\n",
    "                                                              num_sampled=self.num_neg * self.batch_size,\n",
    "                                                              unique=True, distortion=0.75, \n",
    "                                                              range_max=self.vocab, unigrams=self.voc2freq)\n",
    "        neg_ids = tf.cast(neg_ids, dtype=tf.int32)\n",
    "        neg_ids = tf.reshape(neg_ids, [self.batch_size, self.num_neg])\n",
    "        neg_ids = tf.reshape(tf.tile(neg_ids, [1, seq_len]), [self.batch_size, seq_len, self.num_neg])        \n",
    "        # Concatenate true word and negative samples.\n",
    "        target_ind = tf.concat([tf.expand_dims(words_pad, axis=2), neg_ids], axis=2)\n",
    "        # Assign true labels.\n",
    "        target_labels = tf.concat([tf.ones([self.batch_size, seq_len, 1], dtype=tf.float32), \n",
    "                                   tf.zeros([self.batch_size, seq_len, self.num_neg], dtype=tf.float32)], \n",
    "                                  axis=2) * tf.expand_dims(words_mask, -1)\n",
    "        \n",
    "        # -- Run model -- #\n",
    "        gcn_out = self.gcn(self.embeddings(words_pad), adj_mat)\n",
    "        target_embed = self.context(target_ind)\n",
    "        pred = tf.reduce_sum(tf.expand_dims(gcn_out, axis=2) * \n",
    "                             target_embed, axis=3) * tf.expand_dims(words_mask, -1)        \n",
    "        # Calculate loss.\n",
    "        loss = self.bin_loss(tf.reshape(target_labels, -1), tf.reshape(pred, -1))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def checkpoint(self):\n",
    "        embed_matrix = tf.math.l2_normalize(self.embeddings.weights[0], axis=1)\n",
    "        words = [self.id2voc[i] for i in range(len(self.id2voc))]\n",
    "        voc2vec = dict(zip(words, iter(embed_matrix.numpy())))\n",
    "        embedding = Embedding.from_dict(voc2vec)\n",
    "        results = evaluate_on_all(embedding)\n",
    "        results = {key: round(val[0], 4) for key, val in results.items()}\n",
    "        curr_int = np.mean(list(results.values()))\n",
    "        # self.logger.info('Current Score: {}'.format(curr_int))\n",
    "\n",
    "        if curr_int >= self.best_int_avg:\n",
    "            with open(f\"{os.getcwd()}/embeddings/{self.output_name}.txt\", 'w') as file:\n",
    "                for key, values in voc2vec.items():\n",
    "                    file.write(key)\n",
    "                    [file.write(f\" {v}\") for v in values]\n",
    "                    file.write('\\n')\n",
    "            self.best_results = results\n",
    "            self.best_int_avg = curr_int\n",
    "        return results\n",
    "        \n",
    "        \n",
    "    def train(self, epochs):\n",
    "\n",
    "        self.best_int_avg = 0\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            num_batches = get_corpus_len() // self.batch_size\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}:\\n\")\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "            # Iterate over the batches of the dataset.\n",
    "            for step, (X, y) in enumerate(get_batch()):\n",
    "                # Checkpoint every 100 batches.\n",
    "                if (step) % 100 == 0:\n",
    "                    results = self.checkpoint()\n",
    "                    print(results)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss = model(X)\n",
    "                    loss += tf.reduce_sum(model.losses)\n",
    "\n",
    "                grads = tape.gradient(loss, self.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "                print(f\"\\rStep: {step + 1}/{num_batches}; Elapsed time: {time.time() - start_time:.2f}; \\\n",
    "                      Training loss: {loss:.4f}\", end='\\r') \n",
    "\n",
    "#                     print(f\"Step: {step}/{num_batches}; Training loss: {loss:.4f}\")\n",
    "\n",
    "            # At end of each epoch.\n",
    "            self.checkpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_info_path = f\"{os.getcwd()}/semantic_info\"\n",
    "voc2id_path = f\"{os.getcwd()}/data/voc2id.txt\"\n",
    "emb_init_file = f\"{os.getcwd()}/embeddings/init_syngcn_emb.txt\"\n",
    "model = SynGCN(vocab, p.embed_dim, num_deLabel, id2voc, voc2freq, sem_info_path, voc2id_path, \n",
    "                 batch_size=128, emb_init_file=emb_init_file, gcn_gating=True,\n",
    "                 gamma=0.0, reg_sample_size=100000, num_neg=200, name=\"syngcn_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
