{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models import Model\n",
    "from helper import *\n",
    "import tensorflow as tf, time, ctypes\n",
    "from sparse import COO\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Softmax, ReLU, Flatten, Dense\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "\n",
    "# from web.embedding import Embedding\n",
    "# from web.evaluate  import evaluate_on_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.gpu = '0'\n",
    "        self.name = 'test_run'\n",
    "        self.embed_loc = None\n",
    "        self.embed_dim = 300\n",
    "        self.total_sents = 56974869\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = 128\n",
    "        self.max_epochs = 50\n",
    "        self.l2 = 0.00001\n",
    "        self.seed = 1234\n",
    "        self.sample = 1e-4\n",
    "        self.num_neg = 100\n",
    "        self.side_int = 10000\n",
    "        self.gcn_layer = 1\n",
    "        self.dropout = 1.0\n",
    "        self.opt = 'adam'\n",
    "        self.onlyDump = 'store_true'\n",
    "        self.context = False\n",
    "        self.restore ='store_true'\n",
    "        self.emb_dir = './embeddings/'\n",
    "        self.log_dir = './log/'\n",
    "        self.config_dir ='./config/'\n",
    "        self.max_sent_len = 50\n",
    "        self.max_dep_len = 800\n",
    "\n",
    "p = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! g++ batch_generator.cpp -o batchGen.so -fPIC -shared -pthread -O3 -march=native -std=c++11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "\n",
    "voc2id = {k: int(v) for k, v in read_mappings('./data/voc2id.txt').items()}\n",
    "id2freq = id2freq = {int(k): int(v) for k, v in read_mappings('./data/id2freq.txt').items()}\n",
    "id2voc = {v: k for k, v in voc2id.items()}\n",
    "vocab_size = len(voc2id)\n",
    "wrd_list = [id2voc[i] for i in range(vocab_size)]\n",
    "de2id = {k: int(v) for k, v in read_mappings('./data/de2id.txt').items()}\n",
    "num_deLabel = len(de2id)\n",
    "\n",
    "vocab = len(voc2id)\n",
    "corpus_size = np.sum(list(id2freq.values()))\n",
    "voc2freq = [id2freq[_id] for _id in range(len(voc2id))]\n",
    "\n",
    "if not p.context: \n",
    "    p.win_size = 0\n",
    "\n",
    "lib = ctypes.cdll.LoadLibrary('./batchGen.so')\t\t\t# Loads the C++ code for making batches\n",
    "lib.init()\n",
    "\n",
    "# Creating pointers required for creating batches\n",
    "edges = np.zeros(p.max_dep_len * p.batch_size * 3, dtype=np.int32)\n",
    "wrds = np.zeros(p.max_sent_len * p.batch_size, dtype=np.int32)\n",
    "samp = np.zeros(p.max_sent_len * p.batch_size, dtype=np.int32)\n",
    "negs = np.zeros(p.max_sent_len * p.num_neg * p.batch_size, dtype=np.int32)\n",
    "wlen = np.zeros(p.batch_size, dtype=np.int32)\n",
    "elen = np.zeros(p.batch_size, dtype=np.int32)\n",
    "\n",
    "# Pointer address of above arrays\n",
    "edges_addr = edges.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "wrds_addr = wrds.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "negs_addr = negs.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "samp_addr = samp.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "wlen_addr = wlen.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "elen_addr = elen.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batches.\n",
    "\n",
    "# shuffle = True\n",
    "lib.reset()\n",
    "while True:\n",
    "    # max_len = 0; unused variable\n",
    "    eph_ovr = lib.getBatch(edges_addr, wrds_addr, negs_addr, samp_addr, elen_addr, wlen_addr, \n",
    "                    p.win_size, 0, p.batch_size, ctypes.c_float(0))\n",
    "    if eph_ovr == 1: \n",
    "        break\n",
    "    batch = {'edges': edges, 'wrds': wrds, 'elen': elen, 'wlen': wlen}\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj(batch, seq_len):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch: Batch returned by getBatch generator\n",
    "    seq_len: Max length of sentence in the batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Adjacency matrix shape=[Number of dependency labels, Batch size, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "\n",
    "    # Total number of edges in batch.\n",
    "    num_edges = np.sum(batch['elen'])\n",
    "    # Sentence number in batch.\n",
    "    b_ind = np.expand_dims(np.repeat(np.arange(p.batch_size), batch['elen']), axis=1)\n",
    "    # Reshape edges vector to have (parent, dep, label) format\n",
    "    e_ind = np.reshape(batch['edges'], [-1, 3])[:num_edges]\n",
    "\n",
    "    # Indexes to format (label, batch_num, parent, dep).\n",
    "    adj_ind = np.concatenate([b_ind, e_ind], axis=1)\n",
    "    adj_ind = adj_ind[:, [3,0,1,2]]\n",
    "    # Put edge weight 1 for each edge in batch.\n",
    "    adj_data = np.ones(num_edges, dtype=np.float32)\n",
    "\n",
    "    return COO(adj_ind.T, adj_data, shape=(num_deLabel, p.batch_size, seq_len, seq_len)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(data, dlen, sub_sample=[]):\n",
    "\n",
    "    \"\"\"\n",
    "    Pads a given batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: List of tokenized sentences in a batch\n",
    "    dlen: Total number of words in each sentence in a batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_pad: Padded word sequence\n",
    "    data_mask: Masking for padded words\n",
    "    max_len: Maximum length of sentence in the batch\n",
    "    \"\"\"\n",
    "    \n",
    "    max_len   = np.max(dlen)\n",
    "    data_pad  = np.zeros([len(dlen), max_len], dtype=np.int32)\n",
    "    data_mask = np.zeros([len(dlen), max_len], dtype=np.float32)\n",
    "\n",
    "    offset = 0\n",
    "    for i in range(len(dlen)):\n",
    "        data_pad [i, :dlen[i]] = data[offset: offset + dlen[i]]\n",
    "        data_mask[i, :dlen[i]] = 1\n",
    "        if len(sub_sample) != 0:\n",
    "            data_mask[i, :dlen[i]] *= sub_sample[offset: offset + dlen[i]]\n",
    "        offset += dlen[i]\n",
    "\n",
    "    return data_pad, data_mask, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Layer):\n",
    "\n",
    "    def __init__(self, out_dim, n_labels, batch_size=128, gating=False, name=None):\n",
    "\n",
    "        super(GCN, self).__init__(name=name)\n",
    "        \n",
    "        self.out_dim = out_dim\n",
    "        self.n_labels = n_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.gating = gating\n",
    "        \n",
    "        self.in_layers = list()\n",
    "        self.out_layers = list()\n",
    "        self.in_gates = list()\n",
    "        self.out_gates = list()\n",
    "        \n",
    "        for i in range(self.n_labels):\n",
    "            self.in_layers.append(Dense(self.out_dim, kernel_regularizer='l2', name=f\"in_{i}\"))\n",
    "            self.out_layers.append(Dense(self.out_dim, kernel_regularizer='l2', name=f\"out_{i}\"))\n",
    "            self.in_gates.append(Dense(1, activation=\"sigmoid\", name=f\"in_gate_{i}\"))\n",
    "            self.out_gates.append(Dense(1, activation=\"sigmoid\", name=f\"out_gate_{i}\"))\n",
    "\n",
    "\n",
    "    def call(self, gcn_input, adj_mat):\n",
    "\n",
    "        # Rolling sum of adjacent nodes.\n",
    "        max_nodes = adj_mat.shape[-1]\n",
    "        adj_mat = tf.cast(adj_mat, dtype=tf.float32)\n",
    "        out = tf.zeros([self.batch_size, max_nodes, self.out_dim])\n",
    "        \n",
    "        for i in range(self.n_labels):\n",
    "            # In degrees.\n",
    "            xW_in = self.in_layers[i](gcn_input)\n",
    "            A_in = tf.transpose(adj_mat[i], [0, 2, 1])\n",
    "            if self.gating: \n",
    "                xW_in = xW_in * self.in_gates[i](gcn_input)           \n",
    "            h_in = tf.matmul(A_in, xW_in)\n",
    "            \n",
    "            # Out degrees.\n",
    "            xW_out = self.out_layers[i](gcn_input)\n",
    "            A_out = adj_mat[i]\n",
    "            if self.gating: \n",
    "                xW_out = xW_out * self.out_gates[i](gcn_input)\n",
    "            h_out = tf.matmul(A_out, xW_out)\n",
    "            \n",
    "            # Total\n",
    "            out += (h_in + h_out)\n",
    "            \n",
    "        return out\n",
    "        \n",
    "     \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemRegularizer(Regularizer):\n",
    "    def __init__(self, sem_info_path, voc2id_path, \n",
    "                 gamma=0.1, sample_size=1000):\n",
    "        \n",
    "        self.sem_info_path = sem_info_path\n",
    "        self.voc2id_path = voc2id_path\n",
    "        self.gamma = gamma\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        self.voc2id = dict()\n",
    "        self._load_voc2id()\n",
    "        self.syn = self._load_file(\"synonyms\")\n",
    "        self.ant = self._load_file(\"antonyms\")\n",
    "        self.hyper = self._load_file(\"hypernyms\")\n",
    "        self.hypo = self._load_file(\"hyponyms\")\n",
    "        self.mer = self._load_file(\"meronyms\")\n",
    "\n",
    "        self.n_samples = len(self.syn) + len(self.ant) + len(self.hyper) + len(self.hypo) + len(self.mer)\n",
    "        if self.sample_size > self.n_samples: \n",
    "            self.sample_size = self.n_samples\n",
    "        self.n_syn = int(len(self.syn) / self.n_samples * self.sample_size)\n",
    "        self.n_ant = int(len(self.ant) / self.n_samples * self.sample_size)\n",
    "        self.n_hyper = int(len(self.hyper) / self.n_samples * self.sample_size)\n",
    "        self.n_hypo = int(len(self.hypo) / self.n_samples * self.sample_size)\n",
    "        self.n_mer = int(len(self.mer) / self.n_samples * self.sample_size)\n",
    "        \n",
    "        \n",
    "    def _load_voc2id(self):\n",
    "        with open(self.voc2id_path, 'r') as file:\n",
    "            for line in file:\n",
    "                x = line.strip('\\n').split('\\t')\n",
    "                self.voc2id[x[0]] = int(x[1])        \n",
    "    \n",
    "    def _load_file(self, name):\n",
    "        nym = list()\n",
    "        with open(f\"{self.sem_info_path}/{name}.txt\", 'r') as file:\n",
    "            for line in file:\n",
    "                x = line.strip('\\n').split()\n",
    "                inds = list()\n",
    "                for i in x:\n",
    "                    try: inds.append(self.voc2id[i])\n",
    "                    except KeyError: pass\n",
    "                for i in itertools.combinations(inds, 2):\n",
    "                    nym.append(i)\n",
    "            return np.asarray(nym)        \n",
    "            \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # Synonyms\n",
    "        ind = np.random.randint(0, len(self.syn), size=(self.n_syn,))\n",
    "        y = tf.gather(x, self.syn[ind], axis=0)\n",
    "        syn_sim = 1 + tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "\n",
    "        # Antonyms\n",
    "        ind = np.random.randint(0, len(self.ant), size=(self.n_ant,))\n",
    "        y = tf.gather(x, self.ant[ind], axis=0)\n",
    "        ant_sim = 1 - tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        \n",
    "        # Hypernyms\n",
    "        ind = np.random.randint(0, len(self.hyper), size=(self.n_hyper,))\n",
    "        y = tf.gather(x, self.hyper[ind], axis=0)\n",
    "        hyper_sim = 1 + tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        \n",
    "        # Hyponyms\n",
    "        ind = np.random.randint(0, len(self.hypo), size=(self.n_hypo,))\n",
    "        y = tf.gather(x, self.hypo[ind], axis=0)\n",
    "        hypo_sim = (1 + tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1])))\n",
    "        \n",
    "        # Meronyms\n",
    "        ind = np.random.randint(0, len(self.mer), size=(self.n_mer,))\n",
    "        y = tf.gather(x, self.mer[ind], axis=0)\n",
    "        mer_sim = (1 + tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1])))\n",
    "\n",
    "        total = (syn_sim + ant_sim + hyper_sim + hypo_sim + mer_sim) / self.n_samples\n",
    "        \n",
    "        return self.gamma * total\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'gamma': float(self.gamma), \n",
    "                'sample_size': int(self.sample_size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab, dims, init_file=None, kernel_regularizer=None, name=None):\n",
    "        \n",
    "        super(Embedding, self).__init__(name=name)\n",
    "        self.vocab = vocab\n",
    "        self.dims = dims\n",
    "        self.init_file = init_file\n",
    "        \n",
    "        self.w = self.add_weight(shape=(self.vocab, self.dims),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True, \n",
    "                                 regularizer=kernel_regularizer)\n",
    "\n",
    "        if self.init_file != None:            \n",
    "            x = tf.Variable(initial_value=np.asarray(pd.read_csv(self.init_file, sep=' ', header=None).iloc[:, 1:], dtype=\"float32\"), \n",
    "                                 dtype=tf.float32, trainable=True)\n",
    "            self.w.assign(x)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.w, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynGCN(Model):\n",
    "\n",
    "    def __init__(self, vocab, dim, num_deLabel, voc2freq, sem_info_path, voc2id_path, \n",
    "                 batch_size=128, emb_init_file=None, gcn_gating=True,\n",
    "                 gamma=0.1, reg_sample_size=1000, num_neg=100):\n",
    "                \n",
    "        super(SynGCN, self).__init__()\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.dim = dim\n",
    "        self.sem_info_path = sem_info_path\n",
    "        self.voc2id_path = voc2id_path\n",
    "        self.emb_init_file = emb_init_file\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.reg_sample_size = reg_sample_size\n",
    "        self.num_deLabel = num_deLabel\n",
    "        self.gcn_gating=gcn_gating\n",
    "        self.voc2freq = voc2freq\n",
    "        self.num_neg = num_neg\n",
    "        \n",
    "        self.sem_reg = SemRegularizer(self.sem_info_path, self.voc2id_path, \n",
    "                                      gamma=self.gamma, sample_size=self.reg_sample_size)\n",
    "        self.embeddings = Embedding(self.vocab, self.dim, init_file=self.emb_init_file, \n",
    "                                    kernel_regularizer=self.sem_reg, name=\"word_embeddings\")\n",
    "        self.gcn = GCN(self.dim, self.num_deLabel, batch_size=self.batch_size, gating=self.gcn_gating)\n",
    "        self.relu = ReLU()\n",
    "        self.context = Embedding(self.vocab, self.dim, init_file=None, kernel_regularizer='l2', \n",
    "                                 name=\"context_embeddings\")        \n",
    "        self.bin_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        \n",
    "    def call(self, batch):\n",
    "        \n",
    "        # -- Preprocess batch data -- #\n",
    "        # Pad sentences and sample words mask.\n",
    "        words_pad, words_mask, seq_len = pad_data(batch['wrds'], batch['wlen'])\n",
    "        # Gold label indices.\n",
    "        target_words = tf.cast(tf.reshape(words_pad, [-1, 1]), tf.int64)\n",
    "        # Get adjacency matrix.\n",
    "        adj_mat = get_adj(batch, seq_len)        \n",
    "        # Get negative samples.\n",
    "        neg_ids, _, _ = tf.nn.fixed_unigram_candidate_sampler(true_classes=target_words, num_true=1,\n",
    "                                                              num_sampled=self.num_neg * self.batch_size,\n",
    "                                                              unique=True, distortion=0.75, \n",
    "                                                              range_max=self.vocab, unigrams=self.voc2freq)\n",
    "        neg_ids = tf.cast(neg_ids, dtype=tf.int32)\n",
    "        neg_ids = tf.reshape(neg_ids, [self.batch_size, self.num_neg])\n",
    "        neg_ids = tf.reshape(tf.tile(neg_ids, [1, seq_len]), [self.batch_size, seq_len, self.num_neg])        \n",
    "        # Concatenate true word and negative samples.\n",
    "        target_ind = tf.concat([tf.expand_dims(words_pad, axis=2), neg_ids], axis=2)\n",
    "        # Assign true labels.\n",
    "        target_labels = tf.concat([tf.ones([self.batch_size, seq_len, 1], dtype=tf.float32), \n",
    "                                   tf.zeros([self.batch_size, seq_len, self.num_neg], dtype=tf.float32)], \n",
    "                                  axis=2) * tf.expand_dims(words_mask, -1)\n",
    "        \n",
    "        # -- Run model -- #\n",
    "        gcn_out = self.gcn(self.embeddings(words_pad), adj_mat)\n",
    "        target_embed = self.context(target_ind)\n",
    "        pred = tf.reduce_sum(tf.expand_dims(gcn_out, axis=2) * \n",
    "                             target_embed, axis=3) * tf.expand_dims(words_mask, -1)        \n",
    "        # Calculate loss.\n",
    "        loss = self.bin_loss(tf.reshape(target_labels, -1), tf.reshape(pred, -1))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_info_path = f\"{os.getcwd()}/semantic_info\"\n",
    "voc2id_path = f\"{os.getcwd()}/data/voc2id.txt\"\n",
    "emb_init_file = f\"{os.getcwd()}/embeddings/init_rand_emb.txt\"\n",
    "syn_gcn = SynGCN(vocab, p.embed_dim, num_deLabel, voc2freq, sem_info_path, voc2id_path, \n",
    "                 batch_size=128, emb_init_file=emb_init_file, gcn_gating=True,\n",
    "                 gamma=0.1, reg_sample_size=1000, num_neg=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.6931665>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_gcn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_optimizer(self, loss, isAdam=True):\n",
    "    \"\"\"\n",
    "    Add optimizer for training variables\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    loss:\t\tComputed loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_op:\tTraining optimizer\n",
    "    \"\"\"\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        if isAdam:  optimizer = tf.train.AdamOptimizer(self.p.lr)\n",
    "        else:       optimizer = tf.train.GradientDescentOptimizer(self.p.lr)\n",
    "        train_op  = optimizer.minimize(loss)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "def __init__(self, params):\n",
    "    \"\"\"\n",
    "    Constructor for the main function. Loads data and creates computation graph. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params:\t\tHyperparameters of the model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    self.p = params\n",
    "\n",
    "    if not os.path.isdir(self.p.log_dir): os.system('mkdir {}'.format(self.p.log_dir))\n",
    "    if not os.path.isdir(self.p.emb_dir): os.system('mkdir {}'.format(self.p.emb_dir))\n",
    "\n",
    "    self.logger = get_logger(self.p.name, self.p.log_dir, self.p.config_dir)\n",
    "\n",
    "\n",
    "    self.logger.info(vars(self.p)); pprint(vars(self.p))\n",
    "    self.p.batch_size = self.p.batch_size\n",
    "\n",
    "    if self.p.l2 == 0.0:    self.regularizer = None\n",
    "    else:           \tself.regularizer = tf.contrib.layers.l2_regularizer(scale=self.p.l2)\n",
    "\n",
    "    self.load_data()\n",
    "    self.add_placeholders()\n",
    "\n",
    "    nn_out    = self.add_model()\n",
    "    self.loss = self.add_loss_op(nn_out)\n",
    "\n",
    "    if self.p.opt == 'adam': self.train_op = self.add_optimizer(self.loss)\n",
    "    else:            self.train_op = self.add_optimizer(self.loss, isAdam=False)\n",
    "\n",
    "    self.merged_summ = tf.summary.merge_all()\n",
    "\n",
    "def checkpoint(self, epoch, sess):\n",
    "    \"\"\"\n",
    "    Computes intrinsic scores for embeddings and dumps the embeddings embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch:\t\tCurrent epoch number\n",
    "    sess:\t\tTensorflow session object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    embed_matrix, context_matrix \t= sess.run([self.embed_matrix, self.context_matrix])\n",
    "    voc2vec \t= {wrd: embed_matrix[wid] for wrd, wid in self.voc2id.items()}\n",
    "    embedding \t= Embedding.from_dict(voc2vec)\n",
    "    results\t\t= evaluate_on_all(embedding)\n",
    "    results \t= {key: round(val[0], 4) for key, val in results.items()}\n",
    "    curr_int \t= np.mean(list(results.values()))\n",
    "    self.logger.info('Current Score: {}'.format(curr_int))\n",
    "\n",
    "    if curr_int > self.best_int_avg:\n",
    "        self.logger.info(\"Saving embedding matrix\")\n",
    "        f = open('{}/{}'.format(self.p.emb_dir, self.p.name), 'w')\n",
    "        for id, wrd in self.id2voc.items():\n",
    "            f.write('{} {}\\n'.format(wrd, ' '.join([str(round(v, 6)) for v in embed_matrix[id].tolist()])))\n",
    "\n",
    "        self.saver.save(sess=sess, save_path=self.save_path)\n",
    "        self.best_int_avg = curr_int\n",
    "\n",
    "def run_epoch(self, sess, epoch, shuffle=True):\n",
    "    \"\"\"\n",
    "    Runs one epoch of training\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sess:\t\tTensorflow session object\n",
    "    epoch:\t\tEpoch number\n",
    "    shuffle:\tShuffle data while before creates batches\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss:\t\tLoss over the corpus\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    cnt = 0\n",
    "\n",
    "    st = time.time()\n",
    "    for step, batch in enumerate(self.getBatches(shuffle)):\n",
    "        feed    = self.create_feed_dict(batch)\n",
    "        loss, _ = sess.run([self.loss, self.train_op], feed_dict=feed)\n",
    "        losses.append(loss)\n",
    "        cnt += self.p.batch_size\n",
    "\n",
    "        if (step+1) % 10 == 0:\n",
    "            self.logger.info('E:{} (Sents: {}/{} [{}]): Train Loss \\t{:.5}\\t{}\\t{:.5}'.format(epoch, cnt, self.p.total_sents, round(cnt/self.p.total_sents * 100 , 1), np.mean(losses), self.p.name, self.best_int_avg))\n",
    "            en = time.time()\n",
    "            if (en-st) >= (3600):\n",
    "                self.logger.info(\"One more hour is over\")\n",
    "                self.checkpoint(epoch, sess)\n",
    "                st = time.time()\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "def fit(self, sess):\n",
    "    \"\"\"\n",
    "    Trains the model and finally evaluates on test\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sess:\t\tTensorflow session object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    self.saver       = tf.train.Saver()\n",
    "    save_dir  \t = 'checkpoints/' + self.p.name + '/'\n",
    "    if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "    self.save_path   = os.path.join(save_dir, 'best_int_avg')\n",
    "\n",
    "    self.best_int_avg  = 0.0\n",
    "\n",
    "    if self.p.restore:\n",
    "        self.saver.restore(sess, self.save_path)\n",
    "\n",
    "    for epoch in range(self.p.max_epochs):\n",
    "        self.logger.info('Epoch: {}'.format(epoch))\n",
    "        train_loss = self.run_epoch(sess, epoch)\n",
    "\n",
    "        self.checkpoint(epoch, sess)\n",
    "        self.logger.info('[Epoch {}]: Training Loss: {:.5}, Best Loss: {:.5}\\n'.format(epoch, train_loss,  self.best_int_avg))\n",
    "\n",
    "\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "\n",
    "parser = argparse.ArgumentParser(description='WORD GCN')\n",
    "\n",
    "parser.add_argument('-gpu',      dest=\"gpu\",            default='0',                \thelp='GPU to use')\n",
    "parser.add_argument('-name',     dest=\"name\",           default='test_run',             help='Name of the run')\n",
    "parser.add_argument('-embed',    dest=\"embed_loc\",      default=None,         \t\thelp='Embedding for initialization')\n",
    "parser.add_argument('-embed_dim',dest=\"embed_dim\",      default=300,      type=int,     help='Embedding Dimension')\n",
    "parser.add_argument('-total',    dest=\"total_sents\",    default=56974869, type=int,     help='Total number of sentences in file')\n",
    "parser.add_argument('-lr',       dest=\"lr\",             default=0.001,    type=float,   help='Learning rate')\n",
    "parser.add_argument('-batch',    dest=\"batch_size\",     default=128,      type=int,     help='Batch size')\n",
    "parser.add_argument('-epoch',    dest=\"max_epochs\",     default=50,       type=int,     help='Max epochs')\n",
    "parser.add_argument('-l2',       dest=\"l2\",             default=0.00001,  type=float,   help='L2 regularization')\n",
    "parser.add_argument('-seed',     dest=\"seed\",           default=1234,     type=int,     help='Seed for randomization')\n",
    "parser.add_argument('-sample',\t dest=\"sample\",\t  \tdefault=1e-4,     type=float,   help='Subsampling parameter')\n",
    "parser.add_argument('-neg',      dest=\"num_neg\",    \tdefault=100,      type=int,     help='Number of negative samples')\n",
    "parser.add_argument('-side_int', dest=\"side_int\",    \tdefault=10000,    type=int,     help='Number of negative samples')\n",
    "parser.add_argument('-gcn_layer',dest=\"gcn_layer\",      default=1,        type=int,     help='Number of layers in GCN over dependency tree')\n",
    "parser.add_argument('-drop',     dest=\"dropout\",        default=1.0,      type=float,   help='Dropout for full connected layer (Keep probability')\n",
    "parser.add_argument('-opt',      dest=\"opt\",            default='adam',             \thelp='Optimizer to use for training')\n",
    "parser.add_argument('-dump',  \t dest=\"onlyDump\",       action='store_true',        \thelp='Dump context and embed matrix')\n",
    "parser.add_argument('-context',  dest=\"context\",        action='store_true',        \thelp='Include sequential context edges (default: False)')\n",
    "parser.add_argument('-restore',  dest=\"restore\",        action='store_true',        \thelp='Restore from the previous best saved model')\n",
    "parser.add_argument('-embdir',   dest=\"emb_dir\",        default='./embeddings/',       \thelp='Directory for storing learned embeddings')\n",
    "parser.add_argument('-logdir',   dest=\"log_dir\",        default='./log/',       \thelp='Log directory')\n",
    "parser.add_argument('-config',   dest=\"config_dir\",     default='./config/',        \thelp='Config directory')\n",
    "\n",
    "# Added these two arguments to enable others to personalize the training set. Otherwise, the programme may suffer from memory overflow easily.\n",
    "# It is suggested that the -maxlen be set no larger than 100.\n",
    "parser.add_argument('-maxsentlen',dest=\"max_sent_len\",\tdefault=50, \t  type=int,\thelp='Max length of the sentences in data.txt (default: 40)')\n",
    "parser.add_argument('-maxdeplen', dest=\"max_dep_len\", \tdefault=800,\t  type=int,\thelp='Max length of the dependency relations in data.txt (default: 800)')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "if not args.restore: args.name = args.name + '_' + time.strftime(\"%d_%m_%Y\") + '_' + time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "tf.set_random_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "set_gpu(args.gpu)\n",
    "\n",
    "model = SynGCN(args)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model.fit(sess)\n",
    "\n",
    "print('Model Trained Successfully!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a generator for creating batches\n",
    "Parameters\n",
    "----------\n",
    "shuffle:\tWhether to shuffle batches or not\n",
    "Returns\n",
    "-------\n",
    "A batch in the form of a diciontary\n",
    "    edges:\tDependency parse edges\n",
    "    wrds:\tWord in the batch\n",
    "    negs:\tList of negative samples\n",
    "    sample: Subsampled words indicator\n",
    "    elen:\tTotal number of edges in each sentence\n",
    "    wlen:\tTotal number of words in each sentence\n",
    "\"\"\"\n",
    "lib.reset()\n",
    "\n",
    "while True:\n",
    "    # max_len = 0; unused variable\n",
    "    eph_ovr = lib.getBatch(edges_addr, wrds_addr, negs_addr, samp_addr, elen_addr, wlen_addr, \n",
    "                    3, num_neg, batch_size, ctypes.c_float(1e-4))\n",
    "    if eph_ovr == 1: break\n",
    "    x = {'edges': edges, 'wrds': wrds, 'negs': negs, 'sample': samp, 'elen': elen, 'wlen': wlen}\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0, 26,  5,  1, 11,  5,  2, 23,  5,  3, 34,  5,  4,  7,  7,  6,\n",
       "       11,  5,  7,  9,  9,  8,  7,  7,  9, 38,  9, 10, 13, 13, 11,  2, 13,\n",
       "       12,  7, 10, 13, 16,  5, 14, 10,  3,  0,  3,  3,  1, 21,  3,  2, 15,\n",
       "        6,  4,  2,  6,  5,  7,  3,  6, 16,  3,  7, 10, 12,  8,  0, 10,  9,\n",
       "        7, 12, 10, 11, 12, 11, 27,  3, 12, 17, 12, 13, 38, 15, 14, 15, 17,\n",
       "       15, 15, 17, 16,  2, 12, 17, 16, 17, 18, 13, 20, 19,  2, 18],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['edges'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0, 26,  5,  1, 11,  5,  2, 23,  5,  3, 34,  5,  4,  7,  7,  6,\n",
       "       11,  5,  7,  9,  9,  8,  7,  7,  9, 38,  9, 10, 13, 13, 11,  2, 13,\n",
       "       12,  7, 10, 13, 16,  5, 14, 10], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['edges'][:42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
