{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Dot\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.losses import cosine_similarity, Loss\n",
    "from web.embedding import Embedding\n",
    "from web.evaluate import evaluate_on_all\n",
    "from tensorflow.keras.initializers import Zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates cosine similarity between words based on synonyms, antonyms, hypernyms, and hyponyms.\n",
    "class SemanticSim(object):\n",
    "   \n",
    "    def __init__(self, sem_info_path, voc2id,\n",
    "                 relu_syn=False, relu_ant=False,\n",
    "                 relu_hyper=False, relu_hypo=False,\n",
    "                 relu_mer=False, gamma=0.1, sample_size=1000):\n",
    "        \n",
    "        # Initialize variables.\n",
    "        self.sem_info_path = sem_info_path\n",
    "        self.voc2id = voc2id\n",
    "        self.relu_syn = relu_syn\n",
    "        self.relu_ant = relu_ant\n",
    "        self.relu_hyper = relu_hyper\n",
    "        self.relu_hypo = relu_hypo\n",
    "        self.relu_mer = relu_mer\n",
    "        self.gamma = gamma\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        # Load semantic_info files.\n",
    "        self.syn = self._load_file(\"synonyms\")\n",
    "        self.ant = self._load_file(\"antonyms\")\n",
    "        self.hyper = self._load_file(\"hypernyms\")\n",
    "        self.hypo = self._load_file(\"hyponyms\")\n",
    "        self.mer = self._load_file(\"meronyms\")\n",
    "        \n",
    "        # Calculate sample size for each word class.\n",
    "        self.n_samples = len(self.syn) + len(self.ant) + len(self.hyper) + len(self.hypo) + len(self.mer)\n",
    "        if self.sample_size > self.n_samples: \n",
    "            self.sample_size = self.n_samples\n",
    "        self.n_syn = int(len(self.syn) / self.n_samples * self.sample_size) + 1\n",
    "        self.n_ant = int(len(self.ant) / self.n_samples * self.sample_size) + 1\n",
    "        self.n_hyper = int(len(self.hyper) / self.n_samples * self.sample_size) + 1\n",
    "        self.n_hypo = int(len(self.hypo) / self.n_samples * self.sample_size) + 1\n",
    "        self.n_mer = int(len(self.mer) / self.n_samples * self.sample_size) + 1\n",
    "        self.sample_size = self.n_syn + self.n_ant + self.n_hyper + self.n_hypo + self.n_mer\n",
    "           \n",
    "    def _load_file(self, name):\n",
    "        \n",
    "        # Load files from semantic_info.\n",
    "        nym = list()\n",
    "        with open(f\"{self.sem_info_path}/{name}.txt\", 'r') as file:\n",
    "            for line in file:\n",
    "                x = line.strip('\\n').split()\n",
    "                inds = list()\n",
    "                for i in x:\n",
    "                    try: inds.append(self.voc2id[i])\n",
    "                    except KeyError: pass\n",
    "                for i in itertools.combinations(inds, 2):\n",
    "                    nym.append(i)\n",
    "            return np.asarray(nym)        \n",
    "            \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        '''@param x (tf.Variable): Word embeddings of dimension (N x K).'''\n",
    "        \n",
    "        inds = list()\n",
    "        \n",
    "        # Synonyms\n",
    "        ind = np.random.randint(0, len(self.syn), size=(self.n_syn,))\n",
    "        y = tf.gather(x, self.syn[ind], axis=0)\n",
    "        \n",
    "        # relu_syn=True will only adjust synonyms that have cosine similarity <0 - i.e. adjust outliers.\n",
    "        if self.relu_syn:\n",
    "            syn_sim = tf.reduce_sum(tf.nn.relu(cosine_similarity(y[:, 0], y[:, 1])))\n",
    "        else:\n",
    "            syn_sim = tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        inds.append(self.syn[ind])\n",
    "        \n",
    "        # Antonyms\n",
    "        ind = np.random.randint(0, len(self.ant), size=(self.n_ant,))\n",
    "        y = tf.gather(x, self.ant[ind], axis=0)\n",
    "        if self.relu_ant:\n",
    "            ant_sim = tf.reduce_sum(tf.nn.relu(cosine_similarity(y[:, 0], y[:, 1])))\n",
    "        else:\n",
    "            ant_sim = tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        inds.append(self.ant[ind])\n",
    "        \n",
    "        # Hypernyms\n",
    "        ind = np.random.randint(0, len(self.hyper), size=(self.n_hyper,))\n",
    "        y = tf.gather(x, self.hyper[ind], axis=0)\n",
    "        if self.relu_hyper:\n",
    "            hyper_sim = tf.reduce_sum(tf.nn.relu(cosine_similarity(y[:, 0], y[:, 1])))\n",
    "        else:\n",
    "            hyper_sim = tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        inds.append(self.hyper[ind])\n",
    "        \n",
    "        # Hyponyms\n",
    "        ind = np.random.randint(0, len(self.hypo), size=(self.n_hypo,))\n",
    "        y = tf.gather(x, self.hypo[ind], axis=0)\n",
    "        if self.relu_hypo:\n",
    "            hypo_sim = tf.reduce_sum(tf.nn.relu(cosine_similarity(y[:, 0], y[:, 1])))\n",
    "        else:\n",
    "            hypo_sim = tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        inds.append(self.hypo[ind])\n",
    "        \n",
    "        # Meronyms\n",
    "        ind = np.random.randint(0, len(self.mer), size=(self.n_mer,))\n",
    "        y = tf.gather(x, self.mer[ind], axis=0)\n",
    "        if self.relu_mer:\n",
    "            mer_sim = tf.reduce_sum(tf.nn.relu(cosine_similarity(y[:, 0], y[:, 1])))\n",
    "        else:\n",
    "            mer_sim = tf.reduce_sum(cosine_similarity(y[:, 0], y[:, 1]))\n",
    "        inds.append(self.mer[ind])\n",
    "        \n",
    "        # SEMANTIC LOSS:\n",
    "        if self.relu_syn:\n",
    "            total = (syn_sim + ant_sim + hyper_sim + hypo_sim + mer_sim) / self.sample_size\n",
    "        else:\n",
    "            total = 1 + (syn_sim + ant_sim + hyper_sim + hypo_sim + mer_sim) / self.sample_size\n",
    "        \n",
    "        return inds, self.gamma * total\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'gamma': float(self.gamma), \n",
    "                'sample_size': int(self.sample_size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom embedding layer, which takes pre-trained embeddings and does not limit size of vocabulary.\n",
    "class WordEmbedding(Layer):\n",
    "    def __init__(self, vocab, dims, init_file=None, id2voc=None, kernel_regularizer=None, name=None):\n",
    "        \n",
    "        super(WordEmbedding, self).__init__(name=name)\n",
    "        self.vocab = vocab\n",
    "        self.dims = dims\n",
    "        self.id2voc = id2voc\n",
    "        self.init_file = init_file        \n",
    "        self.init_emb = tf.Variable(Zeros()(shape=(self.vocab, self.dims), dtype=tf.float32), trainable=False)        \n",
    "        self.w = self.add_weight(shape=(self.vocab, self.dims),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True, \n",
    "                                 regularizer=kernel_regularizer,\n",
    "                                 dtype=tf.float32)\n",
    "\n",
    "        x = np.asarray(pd.read_csv(self.init_file, sep=' ', header=None))\n",
    "        word2row = {word: int(i) for i, word in enumerate(x[:, 0])}\n",
    "        order = list()\n",
    "        for i in range(len(self.id2voc)):\n",
    "            try:\n",
    "                order.append(word2row[self.id2voc[i]])\n",
    "            except KeyError:\n",
    "                order.append(word2row[\"UNK\"])\n",
    "        x = x[order, 1:]                \n",
    "        self.init_emb.assign(x)\n",
    "        self.w.assign(x)        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.w, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic correction model.\n",
    "class SemCorrect(Model):\n",
    "\n",
    "    def __init__(self, vocab_size, dim, voc2id, id2voc, emb_init_file, sem_info_path,\n",
    "                 relu_syn=False, relu_ant=False, relu_hyper=False, relu_hypo=False, relu_mer=False,\n",
    "                 batch_size=128, num_batches=100, gamma=0.1, output_name=None, \n",
    "                 log_dir=\"./log/\", config_dir=\"./config/\"):\n",
    "                \n",
    "        super(SemCorrect, self).__init__()\n",
    "        \n",
    "        self.vocab = vocab_size\n",
    "        self.dim = dim\n",
    "        self.voc2id = voc2id\n",
    "        self.id2voc = id2voc\n",
    "        self.sem_info_path = sem_info_path\n",
    "        self.emb_init_file = emb_init_file\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = num_batches\n",
    "        # Regularization hyperparameter.\n",
    "        self.gamma = gamma\n",
    "        self.output_name = output_name\n",
    "        self.log_dir = log_dir\n",
    "        self.config_dir = config_dir\n",
    "        # If true, these will only correct outliers in each class.\n",
    "        self.relu_syn= relu_syn\n",
    "        self.relu_ant= relu_ant\n",
    "        self.relu_hyper= relu_hyper\n",
    "        self.relu_hypo= relu_hypo\n",
    "        self.relu_mer= relu_mer\n",
    "\n",
    "        self.best_results = 0\n",
    "        self.best_int_avg = 0\n",
    "        \n",
    "        logger = get_logger(self.output_name, self.log_dir, self.config_dir)\n",
    "        logger.setLevel(\"ERROR\")\n",
    "        \n",
    "        self.sem_sim = SemanticSim(self.sem_info_path, self.voc2id, relu_syn=self.relu_syn,\n",
    "                                   relu_ant=self.relu_ant, relu_hyper=self.relu_hyper,\n",
    "                                   relu_hypo=self.relu_hypo, relu_mer=self.relu_mer,\n",
    "                                   gamma=self.gamma, sample_size=self.batch_size)\n",
    "        self.embeddings = WordEmbedding(self.vocab, self.dim, init_file=self.emb_init_file, \n",
    "                                        id2voc=self.id2voc, name=\"word_embeddings\")\n",
    "        self.dot = Dot(axes=-1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        inds, sem_loss = model.sem_sim(model.embeddings.weights[0])        \n",
    "        sample_inds = [i for k in inds for j in k for i in j]\n",
    "        \n",
    "        # SYNTACTIC LOSS:\n",
    "        # Pre-trained embeddings.\n",
    "        init_emb = tf.expand_dims(model.embeddings.init_emb, 0)\n",
    "        # Pre-trained and current samples.\n",
    "        init_sample = tf.expand_dims(tf.nn.embedding_lookup(model.embeddings.init_emb, sample_inds), 0)\n",
    "        curr_sample = tf.expand_dims(model.embeddings(sample_inds), 0)\n",
    "        \n",
    "        # Cosine similairty between dot(current (sample) embeddings and all initial embedding)\n",
    "        # and dot(initial (sample) embeddings and all initial embeddings)\n",
    "        syn_loss = 1 + tf.reduce_mean(cosine_similarity(model.dot([curr_sample, init_emb]), \n",
    "                                                        model.dot([init_sample, init_emb])))        \n",
    "        # TOTAL LOSS\n",
    "        loss = sem_loss + syn_loss\n",
    "        return loss\n",
    "\n",
    "    def checkpoint(self):\n",
    "        # Save model with best average score across sample of easy-to-evaluate tasks.\n",
    "        \n",
    "        embed_matrix = tf.math.l2_normalize(self.embeddings.weights[0], axis=1)\n",
    "        words = [self.id2voc[i] for i in range(len(self.id2voc))]\n",
    "        voc2vec = dict(zip(words, iter(embed_matrix.numpy())))\n",
    "        embedding = Embedding.from_dict(voc2vec)\n",
    "        results = evaluate_on_all(embedding)\n",
    "        results = {key: round(val[0], 4) for key, val in results.items()}\n",
    "        curr_int = np.mean(list(results.values()))\n",
    "\n",
    "        if curr_int >= self.best_int_avg:\n",
    "            with open(f\"{os.getcwd()}/embeddings/{self.output_name}.txt\", 'w') as file:\n",
    "                for key, values in voc2vec.items():\n",
    "                    file.write(key)\n",
    "                    [file.write(f\" {v}\") for v in values]\n",
    "                    file.write('\\n')\n",
    "            self.best_results = results\n",
    "            self.best_int_avg = curr_int\n",
    "        return results\n",
    "        \n",
    "        \n",
    "    def train(self, epochs):\n",
    "        \n",
    "        # Train model.\n",
    "        \n",
    "        self.best_int_avg = 0\n",
    "        for epoch in range(epochs):\n",
    "            print(\"\\nCreating checkpoint:\")\n",
    "            results = self.checkpoint()\n",
    "            print(results)\n",
    "            start_time = time.time()\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}:\\n\")\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "            # Iterate over the batches of the dataset.\n",
    "            for step in range(self.num_batches):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss = model(0)\n",
    "                grads = tape.gradient(loss, self.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "                print(f\"\\rStep: {step + 1}/{self.num_batches}; Elapsed time: {time.time() - start_time:.2f}; \\\n",
    "                      Training loss: {loss:.4f}\", end='\\r') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "sem_info_path = f\"{os.getcwd()}/semantic_info\"\n",
    "emb_init_file = f\"{os.getcwd()}/embeddings/init_avg_emb.txt\"\n",
    "\n",
    "# Model config.\n",
    "voc2id = dict()\n",
    "i = 0\n",
    "with open(emb_init_file, 'r') as file:\n",
    "    for line in file:\n",
    "        voc2id[line.strip().split(' ')[0]] = i\n",
    "        i += 1\n",
    "id2voc = {v: k for k, v in voc2id.items()}\n",
    "vocab_size = len(voc2id)\n",
    "embed_dim = 300\n",
    "\n",
    "# Initialize model.\n",
    "model = SemCorrect(vocab_size, embed_dim, voc2id, id2voc, emb_init_file, sem_info_path,\n",
    "                   relu_syn=False, relu_ant=False, relu_hyper=False, relu_hypo=False, relu_mer=False,\n",
    "                   batch_size=1000, num_batches=50, gamma=0.1, output_name=\"fin_avg_emb_False_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings from evaluation task. Note: The warnings will not be printed here, but printed to logger.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2562, 'BLESS': 0.425, 'Battig': 0.2128, 'ESSLI_2c': 0.4444, 'ESSLI_2b': 0.675, 'ESSLI_1a': 0.4318, 'WS353': 0.2488, 'WS353R': 0.1394, 'WS353S': 0.3727, 'MEN': 0.1796, 'SimLex999': 0.1637, 'RW': 0.0871, 'RG65': 0.3842, 'MTurk': 0.3246, 'MSR': 0.0072, 'Google': 0.003, 'SemEval2012_2': 0.1001}\n",
      "Epoch 1/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 225.47;                       Training loss: 0.0856\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2239, 'BLESS': 0.285, 'Battig': 0.1667, 'ESSLI_2c': 0.3556, 'ESSLI_2b': 0.575, 'ESSLI_1a': 0.4545, 'WS353': 0.1124, 'WS353R': 0.0799, 'WS353S': 0.1418, 'MEN': 0.1174, 'SimLex999': 0.1179, 'RW': 0.0497, 'RG65': 0.1763, 'MTurk': 0.2254, 'MSR': 0.0025, 'Google': 0.0017, 'SemEval2012_2': 0.0709}\n",
      "Epoch 2/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 224.79;                       Training loss: 0.0737\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2114, 'BLESS': 0.275, 'Battig': 0.1648, 'ESSLI_2c': 0.4222, 'ESSLI_2b': 0.625, 'ESSLI_1a': 0.3636, 'WS353': 0.1137, 'WS353R': 0.0847, 'WS353S': 0.183, 'MEN': 0.1553, 'SimLex999': 0.1095, 'RW': 0.0457, 'RG65': 0.252, 'MTurk': 0.3003, 'MSR': 0.0039, 'Google': 0.0017, 'SemEval2012_2': 0.065}\n",
      "Epoch 3/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 224.12;                       Training loss: 0.0612\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2289, 'BLESS': 0.28, 'Battig': 0.1608, 'ESSLI_2c': 0.3778, 'ESSLI_2b': 0.675, 'ESSLI_1a': 0.3864, 'WS353': 0.136, 'WS353R': 0.101, 'WS353S': 0.2146, 'MEN': 0.1543, 'SimLex999': 0.1052, 'RW': 0.0642, 'RG65': 0.3433, 'MTurk': 0.353, 'MSR': 0.0036, 'Google': 0.0016, 'SemEval2012_2': 0.0806}\n",
      "Epoch 4/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 223.28;                       Training loss: 0.0606\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2264, 'BLESS': 0.27, 'Battig': 0.1583, 'ESSLI_2c': 0.4444, 'ESSLI_2b': 0.7, 'ESSLI_1a': 0.4091, 'WS353': 0.1464, 'WS353R': 0.0918, 'WS353S': 0.2263, 'MEN': 0.1553, 'SimLex999': 0.114, 'RW': 0.0617, 'RG65': 0.1529, 'MTurk': 0.3262, 'MSR': 0.0065, 'Google': 0.0021, 'SemEval2012_2': 0.0752}\n",
      "Epoch 5/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 223.78;                       Training loss: 0.0545\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2189, 'BLESS': 0.285, 'Battig': 0.16, 'ESSLI_2c': 0.4, 'ESSLI_2b': 0.65, 'ESSLI_1a': 0.4545, 'WS353': 0.1907, 'WS353R': 0.13, 'WS353S': 0.3082, 'MEN': 0.1609, 'SimLex999': 0.1327, 'RW': 0.0438, 'RG65': 0.3437, 'MTurk': 0.3293, 'MSR': 0.0061, 'Google': 0.0021, 'SemEval2012_2': 0.0885}\n",
      "Epoch 6/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 223.43;                       Training loss: 0.0529\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2413, 'BLESS': 0.26, 'Battig': 0.1656, 'ESSLI_2c': 0.4222, 'ESSLI_2b': 0.675, 'ESSLI_1a': 0.4091, 'WS353': 0.2268, 'WS353R': 0.1813, 'WS353S': 0.3281, 'MEN': 0.1695, 'SimLex999': 0.1315, 'RW': 0.059, 'RG65': 0.341, 'MTurk': 0.3299, 'MSR': 0.0052, 'Google': 0.0022, 'SemEval2012_2': 0.067}\n",
      "Epoch 7/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 222.70;                       Training loss: 0.0486\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2587, 'BLESS': 0.27, 'Battig': 0.165, 'ESSLI_2c': 0.4444, 'ESSLI_2b': 0.625, 'ESSLI_1a': 0.4091, 'WS353': 0.1627, 'WS353R': 0.089, 'WS353S': 0.2704, 'MEN': 0.1768, 'SimLex999': 0.1723, 'RW': 0.0674, 'RG65': 0.3558, 'MTurk': 0.3449, 'MSR': 0.0054, 'Google': 0.0031, 'SemEval2012_2': 0.0726}\n",
      "Epoch 8/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 222.11;                       Training loss: 0.0473\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2537, 'BLESS': 0.27, 'Battig': 0.1627, 'ESSLI_2c': 0.4, 'ESSLI_2b': 0.725, 'ESSLI_1a': 0.3864, 'WS353': 0.2203, 'WS353R': 0.1748, 'WS353S': 0.3071, 'MEN': 0.1747, 'SimLex999': 0.1602, 'RW': 0.0656, 'RG65': 0.2458, 'MTurk': 0.3699, 'MSR': 0.0061, 'Google': 0.0026, 'SemEval2012_2': 0.0794}\n",
      "Epoch 9/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 222.44;                       Training loss: 0.0405\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2587, 'BLESS': 0.31, 'Battig': 0.1648, 'ESSLI_2c': 0.4222, 'ESSLI_2b': 0.625, 'ESSLI_1a': 0.4318, 'WS353': 0.2429, 'WS353R': 0.1722, 'WS353S': 0.3607, 'MEN': 0.171, 'SimLex999': 0.1731, 'RW': 0.0708, 'RG65': 0.2489, 'MTurk': 0.3864, 'MSR': 0.006, 'Google': 0.0036, 'SemEval2012_2': 0.083}\n",
      "Epoch 10/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 221.22;                       Training loss: 0.0408\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2338, 'BLESS': 0.295, 'Battig': 0.1615, 'ESSLI_2c': 0.4444, 'ESSLI_2b': 0.7, 'ESSLI_1a': 0.4091, 'WS353': 0.2486, 'WS353R': 0.1788, 'WS353S': 0.3691, 'MEN': 0.1601, 'SimLex999': 0.176, 'RW': 0.0673, 'RG65': 0.3155, 'MTurk': 0.3493, 'MSR': 0.0068, 'Google': 0.0041, 'SemEval2012_2': 0.0888}\n",
      "Epoch 11/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 222.01;                       Training loss: 0.0411\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2662, 'BLESS': 0.27, 'Battig': 0.1625, 'ESSLI_2c': 0.4222, 'ESSLI_2b': 0.65, 'ESSLI_1a': 0.4545, 'WS353': 0.2236, 'WS353R': 0.1462, 'WS353S': 0.3373, 'MEN': 0.1732, 'SimLex999': 0.205, 'RW': 0.0643, 'RG65': 0.3734, 'MTurk': 0.3426, 'MSR': 0.0066, 'Google': 0.0045, 'SemEval2012_2': 0.0641}\n",
      "Epoch 12/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 222.21;                       Training loss: 0.0417\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2562, 'BLESS': 0.315, 'Battig': 0.1594, 'ESSLI_2c': 0.4444, 'ESSLI_2b': 0.725, 'ESSLI_1a': 0.4318, 'WS353': 0.2262, 'WS353R': 0.135, 'WS353S': 0.3474, 'MEN': 0.1828, 'SimLex999': 0.2034, 'RW': 0.0646, 'RG65': 0.4534, 'MTurk': 0.3599, 'MSR': 0.0078, 'Google': 0.0052, 'SemEval2012_2': 0.0914}\n",
      "Epoch 13/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 222.17;                       Training loss: 0.0394\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2438, 'BLESS': 0.31, 'Battig': 0.165, 'ESSLI_2c': 0.4444, 'ESSLI_2b': 0.675, 'ESSLI_1a': 0.4545, 'WS353': 0.2085, 'WS353R': 0.1254, 'WS353S': 0.3537, 'MEN': 0.188, 'SimLex999': 0.1944, 'RW': 0.0709, 'RG65': 0.2954, 'MTurk': 0.347, 'MSR': 0.0081, 'Google': 0.0073, 'SemEval2012_2': 0.0677}\n",
      "Epoch 14/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 221.68;                       Training loss: 0.0362\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2413, 'BLESS': 0.355, 'Battig': 0.1638, 'ESSLI_2c': 0.4222, 'ESSLI_2b': 0.675, 'ESSLI_1a': 0.4545, 'WS353': 0.211, 'WS353R': 0.1503, 'WS353S': 0.33, 'MEN': 0.1759, 'SimLex999': 0.2054, 'RW': 0.0719, 'RG65': 0.3732, 'MTurk': 0.3644, 'MSR': 0.0078, 'Google': 0.0064, 'SemEval2012_2': 0.0603}\n",
      "Epoch 15/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 220.99;                       Training loss: 0.0378\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2637, 'BLESS': 0.34, 'Battig': 0.1625, 'ESSLI_2c': 0.4444, 'ESSLI_2b': 0.65, 'ESSLI_1a': 0.4318, 'WS353': 0.2619, 'WS353R': 0.1998, 'WS353S': 0.3806, 'MEN': 0.1785, 'SimLex999': 0.2204, 'RW': 0.0766, 'RG65': 0.3834, 'MTurk': 0.3381, 'MSR': 0.0088, 'Google': 0.0062, 'SemEval2012_2': 0.0747}\n",
      "Epoch 16/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 220.96;                       Training loss: 0.0337\n",
      "Creating checkpoint:\n",
      "{'AP': 0.291, 'BLESS': 0.32, 'Battig': 0.1583, 'ESSLI_2c': 0.4444, 'ESSLI_2b': 0.65, 'ESSLI_1a': 0.5, 'WS353': 0.2458, 'WS353R': 0.1596, 'WS353S': 0.3666, 'MEN': 0.19, 'SimLex999': 0.2125, 'RW': 0.0741, 'RG65': 0.3435, 'MTurk': 0.3862, 'MSR': 0.008, 'Google': 0.0071, 'SemEval2012_2': 0.0817}\n",
      "Epoch 17/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 221.84;                       Training loss: 0.0324\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2736, 'BLESS': 0.325, 'Battig': 0.1638, 'ESSLI_2c': 0.4667, 'ESSLI_2b': 0.65, 'ESSLI_1a': 0.5, 'WS353': 0.2502, 'WS353R': 0.1689, 'WS353S': 0.3537, 'MEN': 0.1849, 'SimLex999': 0.2163, 'RW': 0.0767, 'RG65': 0.3853, 'MTurk': 0.3628, 'MSR': 0.0086, 'Google': 0.0074, 'SemEval2012_2': 0.0841}\n",
      "Epoch 18/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 222.96;                       Training loss: 0.0313\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2662, 'BLESS': 0.34, 'Battig': 0.1644, 'ESSLI_2c': 0.4667, 'ESSLI_2b': 0.675, 'ESSLI_1a': 0.5227, 'WS353': 0.2494, 'WS353R': 0.1482, 'WS353S': 0.3824, 'MEN': 0.1904, 'SimLex999': 0.2413, 'RW': 0.0723, 'RG65': 0.3949, 'MTurk': 0.3661, 'MSR': 0.01, 'Google': 0.0075, 'SemEval2012_2': 0.08}\n",
      "Epoch 19/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 223.28;                       Training loss: 0.0306\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2512, 'BLESS': 0.34, 'Battig': 0.1631, 'ESSLI_2c': 0.4222, 'ESSLI_2b': 0.7, 'ESSLI_1a': 0.4773, 'WS353': 0.2286, 'WS353R': 0.1517, 'WS353S': 0.3521, 'MEN': 0.1912, 'SimLex999': 0.2239, 'RW': 0.076, 'RG65': 0.3979, 'MTurk': 0.3631, 'MSR': 0.0102, 'Google': 0.0074, 'SemEval2012_2': 0.0615}\n",
      "Epoch 20/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 221.85;                       Training loss: 0.0301\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2811, 'BLESS': 0.34, 'Battig': 0.1617, 'ESSLI_2c': 0.4444, 'ESSLI_2b': 0.65, 'ESSLI_1a': 0.5, 'WS353': 0.2059, 'WS353R': 0.1247, 'WS353S': 0.3318, 'MEN': 0.187, 'SimLex999': 0.2227, 'RW': 0.0764, 'RG65': 0.3662, 'MTurk': 0.37, 'MSR': 0.0086, 'Google': 0.0092, 'SemEval2012_2': 0.0618}\n",
      "Epoch 21/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 221.24;                       Training loss: 0.0302\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2736, 'BLESS': 0.325, 'Battig': 0.161, 'ESSLI_2c': 0.4222, 'ESSLI_2b': 0.65, 'ESSLI_1a': 0.5455, 'WS353': 0.2275, 'WS353R': 0.154, 'WS353S': 0.3561, 'MEN': 0.1875, 'SimLex999': 0.2277, 'RW': 0.0766, 'RG65': 0.3819, 'MTurk': 0.3528, 'MSR': 0.0091, 'Google': 0.0077, 'SemEval2012_2': 0.0695}\n",
      "Epoch 22/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 220.88;                       Training loss: 0.0263\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2662, 'BLESS': 0.33, 'Battig': 0.164, 'ESSLI_2c': 0.4444, 'ESSLI_2b': 0.675, 'ESSLI_1a': 0.4773, 'WS353': 0.2002, 'WS353R': 0.1427, 'WS353S': 0.3325, 'MEN': 0.1817, 'SimLex999': 0.2259, 'RW': 0.072, 'RG65': 0.3251, 'MTurk': 0.371, 'MSR': 0.0082, 'Google': 0.0118, 'SemEval2012_2': 0.0689}\n",
      "Epoch 23/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 220.76;                       Training loss: 0.0283\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2711, 'BLESS': 0.365, 'Battig': 0.1652, 'ESSLI_2c': 0.4444, 'ESSLI_2b': 0.65, 'ESSLI_1a': 0.4318, 'WS353': 0.1954, 'WS353R': 0.124, 'WS353S': 0.3277, 'MEN': 0.1705, 'SimLex999': 0.2188, 'RW': 0.0754, 'RG65': 0.3009, 'MTurk': 0.3794, 'MSR': 0.0099, 'Google': 0.0123, 'SemEval2012_2': 0.0707}\n",
      "Epoch 24/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 221.86;                       Training loss: 0.0268\n",
      "Creating checkpoint:\n",
      "{'AP': 0.2687, 'BLESS': 0.325, 'Battig': 0.164, 'ESSLI_2c': 0.4222, 'ESSLI_2b': 0.65, 'ESSLI_1a': 0.4545, 'WS353': 0.2012, 'WS353R': 0.1369, 'WS353S': 0.3566, 'MEN': 0.1793, 'SimLex999': 0.2168, 'RW': 0.0701, 'RG65': 0.4147, 'MTurk': 0.3615, 'MSR': 0.0085, 'Google': 0.009, 'SemEval2012_2': 0.0811}\n",
      "Epoch 25/25:\n",
      "\n",
      "Step: 50/50; Elapsed time: 221.63;                       Training loss: 0.0253\r"
     ]
    }
   ],
   "source": [
    "model.train(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
