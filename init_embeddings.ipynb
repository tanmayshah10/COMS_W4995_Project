{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk import RegexpTokenizer\n",
    "from scipy.sparse import lil_matrix, coo_matrix, save_npz, load_npz\n",
    "from scipy.sparse.linalg import svds\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate co-occurence matrix.\n",
    "def gen_coocurrence_matrix(data_file, voc2id_file, de2id_file):\n",
    "\n",
    "    max_voc_id = 0\n",
    "    with open(voc2id_file, 'r') as file:\n",
    "        for line in file:\n",
    "            voc_id = line.strip(\"\\n\").split(\"\\t\")[1]\n",
    "            max_voc_id += 1\n",
    "    max_de_id = 0\n",
    "    with open(de2id_file, 'r') as file:\n",
    "        for line in file:\n",
    "            de_id = line.strip(\"\\n\").split(\"\\t\")[1]\n",
    "            max_de_id += 1\n",
    "\n",
    "    mat = np.zeros((max_de_id, max_voc_id))\n",
    "    cnt = 0\n",
    "    with open(data_file, 'r') as file:\n",
    "        for line in file:\n",
    "            cnt += 1\n",
    "            if cnt % 1000000 == 0:\n",
    "                print(cnt)\n",
    "            info = line.strip('\\n').split()\n",
    "            split_ind = int(info[0]) + 2\n",
    "            tokens = info[2:split_ind]\n",
    "            deps = info[split_ind:]\n",
    "            for d in deps:\n",
    "                ind = d.split('|')\n",
    "                mat[int(ind[2]), int(tokens[int(ind[1])])] += 1\n",
    "\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PMI score for co-occurence matrix.\n",
    "def calc_pmi_score(wc_matrix):\n",
    "    \n",
    "    '''\n",
    "    Inputs:\n",
    "    wc_matrix (scipy.lil_matrix): Word-context count matrix.\n",
    "    \n",
    "    Outputs:\n",
    "    pmi (scipy.lil_matrix): Matrix with pointwise mutual information score for each word-context pair.\n",
    "    '''\n",
    "    \n",
    "    pmi = np.zeros(wc_matrix.shape)\n",
    "    sum_w = wc_matrix.sum(axis=1)\n",
    "    sum_c = wc_matrix.sum(axis=0)\n",
    "    total = wc_matrix.sum()\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    for i in range(pmi.shape[0]):\n",
    "        print(f\"\\rCalculating PMI for row {i}\", end = '')\n",
    "        score = np.log((wc_matrix[i, :] * total) / ((sum_w[i] * sum_c) + 1e-5))\n",
    "        pmi[i, :] = np.maximum(pmi[i, :], score)\n",
    "    np.seterr(divide='warn', invalid='warn')\n",
    "    print(f\"\\rCalculated PMI.\", end = '')\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word embeddings.\n",
    "def gen_dep_embeddings(mat, k=300):\n",
    "    if k > np.min(mat.shape):\n",
    "        dim = np.min(mat.shape)\n",
    "        U, S, _ = svds(mat, k=dim-1)\n",
    "        w_temp = U.dot(np.diag(np.sqrt(S)))\n",
    "        W = np.pad(w_temp, ((0, 0), (0, k - dim + 1)))\n",
    "        return W\n",
    "    else:\n",
    "        U, S, _ = svds(mat, k=k)\n",
    "        # W = U.dot(np.diag(np.sqrt(S)))\n",
    "        return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_word_embeddings(filepath, W, token_index):\n",
    "    \n",
    "    '''\n",
    "    Inputs:\n",
    "    filepath (str): Output filepath with extension.\n",
    "    token_index (dict): Dictionary of {word: index} pairs.\n",
    "    \n",
    "    Outputs:\n",
    "    (None)\n",
    "    '''\n",
    "\n",
    "    with open(filepath, \"w\") as file:\n",
    "        for t in token_index:\n",
    "            embedding = W[token_index[t]]\n",
    "            # Write word.\n",
    "            file.write(f\"{t}\")\n",
    "            # Write embedding.\n",
    "            [file.write(f\" {e}\") for e in embedding]\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_embeddings(n, k):\n",
    "    x = tf.ones(n,)\n",
    "    lamb = 1e3\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(n, k, embeddings_initializer='uniform', input_shape=()))\n",
    "    loss = lambda: -tf.reduce_mean(tf.linalg.matmul(tf.transpose(model(x)), model(x))) + \\\n",
    "                    lamb * tf.reduce_mean(tf.norm(model.trainable_weights[0], ord='euclidean', axis=1) - 1) ** 2\n",
    "    var_list = lambda: model.trainable_weights \n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    for i in range(1000):\n",
    "        opt.minimize(loss, var_list)\n",
    "    return model.trainable_weights[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating PMI for row 0\r",
      "Calculating PMI for row 1\r",
      "Calculating PMI for row 2\r",
      "Calculating PMI for row 3\r",
      "Calculating PMI for row 4\r",
      "Calculating PMI for row 5\r",
      "Calculating PMI for row 6\r",
      "Calculating PMI for row 7\r",
      "Calculating PMI for row 8\r",
      "Calculating PMI for row 9\r",
      "Calculating PMI for row 10\r",
      "Calculating PMI for row 11\r",
      "Calculating PMI for row 12\r",
      "Calculating PMI for row 13\r",
      "Calculating PMI for row 14\r",
      "Calculating PMI for row 15\r",
      "Calculating PMI for row 16\r",
      "Calculating PMI for row 17\r",
      "Calculating PMI for row 18\r",
      "Calculating PMI for row 19\r",
      "Calculating PMI for row 20\r",
      "Calculating PMI for row 21\r",
      "Calculating PMI for row 22\r",
      "Calculating PMI for row 23\r",
      "Calculating PMI for row 24\r",
      "Calculating PMI for row 25\r",
      "Calculating PMI for row 26\r",
      "Calculating PMI for row 27\r",
      "Calculating PMI for row 28\r",
      "Calculating PMI for row 29\r",
      "Calculating PMI for row 30\r",
      "Calculating PMI for row 31\r",
      "Calculating PMI for row 32\r",
      "Calculating PMI for row 33\r",
      "Calculating PMI for row 34\r",
      "Calculating PMI for row 35\r",
      "Calculating PMI for row 36\r",
      "Calculating PMI for row 37\r",
      "Calculating PMI for row 38\r",
      "Calculating PMI for row 39\r",
      "Calculated PMI."
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "data_path = f\"{os.getcwd()}/data\"\n",
    "data_file = f\"{data_path}/data.txt\"\n",
    "voc2id_file = f\"{data_path}/voc2id.txt\"\n",
    "de2id_file = f\"{data_path}/de2id.txt\"\n",
    "dims = 300\n",
    "\n",
    "voc2id = dict()\n",
    "with open(voc2id_file, 'r') as file:\n",
    "    for line in file:\n",
    "        info = line.strip('\\n').split(\"\\t\")\n",
    "        voc2id[info[0]] = int(info[1])\n",
    "\n",
    "print(\"1\")\n",
    "wc_mat= gen_coocurrence_matrix(data_file, voc2id_file, de2id_file)\n",
    "print(\"2\")\n",
    "\n",
    "# PMI embedding.\n",
    "pmi_mat = calc_pmi_score(wc_mat)\n",
    "w = gen_dep_embeddings(pmi_mat)\n",
    "emb = np.dot(wc_mat.T, w) / np.expand_dims(np.sum(wc_mat, axis=0) + 1e-05, axis=-1)\n",
    "out_filepath = os.getcwd() + f\"/embeddings/init_norm_emb.txt\"\n",
    "save_word_embeddings(out_filepath, emb, voc2id)\n",
    "\n",
    "# Average embedding.\n",
    "w = gen_random_embeddings(len(wc_mat), dims)\n",
    "emb = np.dot(wc_mat.T, w) / np.expand_dims(np.sum(wc_mat, axis=0) + 1e-05, axis=-1)\n",
    "out_filepath = os.getcwd() + f\"/embeddings/init_avg_emb.txt\"\n",
    "save_word_embeddings(out_filepath, emb, voc2id)\n",
    "\n",
    "# Random embedding.\n",
    "emb = np.random.uniform(size=(len(voc2id), 300)) - 0.5\n",
    "out_filepath = os.getcwd() + f\"/embeddings/init_rand_emb.txt\"\n",
    "save_word_embeddings(out_filepath, emb, voc2id)\n",
    "\n",
    "t = time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
